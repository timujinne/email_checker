#!/usr/bin/env python3
"""
Smart Filter Processor V2 - –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–æ–≥ Google Script –ª–æ–≥–∏–∫–∏
–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –∏ —Ç–µ–º–∞—Ç–∏–∫–∞–º
–ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
"""

import json
import os
import re
import time
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import shutil
import tempfile

@dataclass
class SmartFilterResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ email"""
    email: str
    original_score: float
    final_score: float
    priority: str
    target_category: str
    exclusion_reasons: List[str]
    indicators: Dict[str, Any]
    metadata: Dict[str, Any]
    processing_timestamp: str

@dataclass
class SmartFilterStatistics:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    total_processed: int = 0
    hard_excluded: int = 0
    valid_emails: int = 0
    qualified_leads: int = 0
    high_priority: int = 0
    medium_priority: int = 0
    low_priority: int = 0
    processing_time: float = 0.0
    errors: int = 0
    excluded_by_category: Dict[str, int] = None

    def __post_init__(self):
        if self.excluded_by_category is None:
            self.excluded_by_category = defaultdict(int)

class SmartFilterLogger:
    """–õ–æ–≥–≥–µ—Ä –¥–ª—è —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""

    def __init__(self, filter_name: str = "smart_filter"):
        self.filter_name = filter_name
        self.logs_dir = Path("logs")
        self.logs_dir.mkdir(exist_ok=True)
        self.log_file = self.logs_dir / f"{filter_name}_{datetime.now().strftime('%Y%m%d')}.log"

    def info(self, message: str, details: str = ""):
        self._log("INFO", message, details)

    def warn(self, message: str, details: str = ""):
        self._log("WARN", message, details)

    def error(self, message: str, details: str = ""):
        self._log("ERROR", message, details)

    def debug(self, message: str, details: str = ""):
        self._log("DEBUG", message, details)

    def _log(self, level: str, message: str, details: str = ""):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {self.filter_name}: {message}"
        if details:
            log_entry += f" - {details}"

        print(log_entry)

        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry + '\n')
        except Exception as e:
            print(f"Failed to write to log file: {e}")

class SmartHardExclusionFilter:
    """–§–∏–ª—å—Ç—Ä –∂–µ—Å—Ç–∫–∏—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–æ–≥ –∏–∑ Google Script"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = SmartFilterLogger("exclusion_filter")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.personal_domains = set(config.get('exclusions', {}).get('personal_domains', []))
        self.hr_prefixes = set(config.get('exclusions', {}).get('hr_prefixes', []))
        self.service_prefixes = set(config.get('exclusions', {}).get('service_prefixes', []))
        self.suspicious_domains = set(config.get('exclusions', {}).get('suspicious_domains', []))
        self.excluded_country_domains = set(config.get('exclusions', {}).get('excluded_country_domains', []))

        self.excluded_industries = config.get('exclusions', {}).get('excluded_industries', {})
        self.suspicious_patterns = [re.compile(pattern) for pattern in
                                  config.get('exclusions', {}).get('suspicious_patterns', [])]

    def should_exclude(self, email: str, company_name: str = "", description: str = "",
                      domain: str = "") -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å email"""
        exclusion_reasons = []

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
            if self.is_personal_domain(email):
                exclusion_reasons.append('personal_domain')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ HR –∏ —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö email
            if self.is_hr_or_service_email(email):
                exclusion_reasons.append('hr_service_prefix')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            if self.is_geographically_excluded(email, description, domain):
                exclusion_reasons.append('geographic_restriction')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            if self.is_suspicious_pattern(email):
                exclusion_reasons.append('suspicious_pattern')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏–Ω–¥—É—Å—Ç—Ä–∏–π
            if self.is_excluded_industry(company_name, description):
                exclusion_reasons.append('excluded_industry')

        except Exception as error:
            self.logger.error("Error in exclusion filter", str(error))
            exclusion_reasons.append('filter_error')

        return {
            'should_exclude': len(exclusion_reasons) > 0,
            'reasons': exclusion_reasons,
            'severity': 'critical' if len(exclusion_reasons) >= 2 else 'warning'
        }

    def is_personal_domain(self, email: str) -> bool:
        if not email or '@' not in email:
            return False

        try:
            domain = email.split('@')[1].lower().strip()
            return domain in self.personal_domains
        except Exception:
            return False

    def is_hr_or_service_email(self, email: str) -> bool:
        if not email:
            return False

        try:
            email_lower = email.lower().strip()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
            for prefix in self.hr_prefixes:
                if email_lower.startswith(prefix):
                    return True

            for prefix in self.service_prefixes:
                if prefix in email_lower:
                    return True

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
            domain = email.split('@')[1].lower().strip()
            for suspicious_domain in self.suspicious_domains:
                if suspicious_domain in domain:
                    return True

        except Exception as error:
            self.logger.error("Error checking HR/service email", str(error))

        return False

    def is_geographically_excluded(self, email: str, description: str = "", domain: str = "") -> bool:
        if not email:
            return False

        try:
            email_domain = email.split('@')[1].lower().strip()
            text = f"{email} {description} {domain}".lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–æ–≤ —Å—Ç—Ä–∞–Ω
            for country_domain in self.excluded_country_domains:
                if email_domain.endswith(country_domain):
                    return True

        except Exception as error:
            self.logger.error("Error checking geographic exclusions", str(error))

        return False

    def is_suspicious_pattern(self, email: str) -> bool:
        if not email:
            return False

        try:
            for pattern in self.suspicious_patterns:
                if pattern.search(email):
                    return True
        except Exception as error:
            self.logger.error("Error checking suspicious patterns", str(error))

        return False

    def is_excluded_industry(self, company_name: str = "", description: str = "") -> bool:
        try:
            text = f"{company_name} {description}".lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏–Ω–¥—É—Å—Ç—Ä–∏–π
            for industry_keywords in self.excluded_industries.values():
                for keyword in industry_keywords:
                    if keyword.lower() in text:
                        return True

        except Exception as error:
            self.logger.error("Error checking excluded industries", str(error))

        return False

class SmartHydraulicDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –≥–∏–¥—Ä–∞–≤–ª–∏—á–µ—Å–∫–∏—Ö —Ü–∏–ª–∏–Ω–¥—Ä–æ–≤ - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–æ–≥ Google Script"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = SmartFilterLogger("hydraulic_detector")

        # Detect keyword structure type
        self.keyword_structure = self._detect_keyword_structure()
        self.logger.info(f"Using keyword structure: {self.keyword_structure}")

        # Normalize keywords to unified format
        self.normalized_keywords = self._normalize_keywords()

        # For backward compatibility - keep old attributes
        self.hydraulic_keywords = self.normalized_keywords.get('primary', [])
        self.application_keywords = self.normalized_keywords.get('applications', [])
        self.component_keywords = self.normalized_keywords.get('secondary', [])
        self.oem_indicators = self.normalized_keywords.get('oem_indicators', [])

        # –î–æ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self.domain_patterns = config.get('domain_patterns', {}).get('relevant_patterns', [])
        self.high_value_domains = config.get('domain_patterns', {}).get('high_value_domains', [])

    def _detect_keyword_structure(self) -> str:
        """
        Detect which keyword structure is used in config

        Returns:
            'flat' - Simple flat structure with keywords.primary/secondary
            'multilingual' - Language-specific groups under industry_keywords
            'unknown' - Unknown or missing structure
        """
        # Check for industry_keywords (multilingual structure)
        if 'industry_keywords' in self.config:
            industry_kw = self.config['industry_keywords']

            # Check if it has language-specific subgroups
            for key, value in industry_kw.items():
                if isinstance(value, dict) and ('primary' in value or 'secondary' in value):
                    return 'multilingual'

            # Has industry_keywords but flat structure inside
            if 'primary' in industry_kw or 'secondary' in industry_kw:
                return 'flat'

        # Check for old-style keywords
        if 'keywords' in self.config:
            return 'flat'

        self.logger.warning("‚ö†Ô∏è  Unknown keyword structure in config! Using empty keywords.")
        print("‚ö†Ô∏è  WARNING: Config does not contain 'keywords' or 'industry_keywords'")
        return 'unknown'

    def _normalize_keywords(self) -> Dict[str, List[str]]:
        """
        Normalize keywords from any structure to unified flat format

        Returns:
            {
                'primary': [...],      # All primary keywords from all languages
                'secondary': [...],    # All secondary keywords
                'oem_indicators': [...],
                'applications': [...],
                'negative': [...]
            }
        """
        normalized = {
            'primary': [],
            'secondary': [],
            'oem_indicators': [],
            'applications': [],
            'negative': []
        }

        if self.keyword_structure == 'flat':
            # Simple flat structure
            source = self.config.get('keywords') or self.config.get('industry_keywords', {})

            normalized['primary'] = source.get('primary', []) or source.get('hydraulic_cylinders', [])
            normalized['secondary'] = source.get('secondary', []) or source.get('components', [])
            normalized['oem_indicators'] = source.get('oem_indicators', [])
            normalized['applications'] = source.get('applications', [])
            normalized['negative'] = source.get('negative_keywords', [])

        elif self.keyword_structure == 'multilingual':
            # Multilingual structure - merge all language groups
            industry_kw = self.config.get('industry_keywords', {})

            for group_name, group_data in industry_kw.items():
                if isinstance(group_data, dict):
                    # Language-specific group (e.g., hydraulic_cylinders_dutch)
                    if 'primary' in group_data or 'secondary' in group_data:
                        normalized['primary'].extend(group_data.get('primary', []))
                        normalized['secondary'].extend(group_data.get('secondary', []))
                        normalized['oem_indicators'].extend(group_data.get('oem_indicators', []))
                        normalized['applications'].extend(group_data.get('applications', []))

                    # Nested language groups (e.g., mobile_hydraulics_keywords.dutch)
                    else:
                        for lang_key, lang_data in group_data.items():
                            if isinstance(lang_data, list):
                                # Add to primary by default
                                normalized['primary'].extend(lang_data)

                elif isinstance(group_data, list):
                    # Direct list (e.g., negative_keywords)
                    if 'negative' in group_name.lower():
                        normalized['negative'].extend(group_data)
                    else:
                        normalized['primary'].extend(group_data)

            # Remove duplicates while preserving order
            for key in normalized:
                seen = set()
                normalized[key] = [x for x in normalized[key] if not (x.lower() in seen or seen.add(x.lower()))]

        # Log statistics
        total_keywords = sum(len(v) for v in normalized.values())
        self.logger.info(f"Normalized {total_keywords} keywords: "
                        f"primary={len(normalized['primary'])}, "
                        f"secondary={len(normalized['secondary'])}, "
                        f"oem={len(normalized['oem_indicators'])}, "
                        f"applications={len(normalized['applications'])}, "
                        f"negative={len(normalized['negative'])}")

        return normalized

    def detect_hydraulic_relevance(self, text: str) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –≥–∏–¥—Ä–∞–≤–ª–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ"""
        score = 0
        found_indicators = []

        try:
            if not text or not isinstance(text, str):
                return self._create_empty_detection_result()

            text_lower = text.lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≥–∏–¥—Ä–∞–≤–ª–∏–∫–∏
            for keyword in self.hydraulic_keywords:
                if keyword.lower() in text_lower:
                    weight = 25 if len(keyword) > 15 else 20 if len(keyword) > 10 else 15
                    score += weight
                    found_indicators.append(f"hydraulic:{keyword}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
            for keyword in self.application_keywords:
                if keyword.lower() in text_lower:
                    weight = 15 if len(keyword) > 10 else 10
                    score += weight
                    found_indicators.append(f"application:{keyword}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            for keyword in self.component_keywords:
                if keyword.lower() in text_lower:
                    weight = 10
                    score += weight
                    found_indicators.append(f"component:{keyword}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ OEM –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            for keyword in self.oem_indicators:
                if keyword.lower() in text_lower:
                    weight = 20
                    score += weight
                    found_indicators.append(f"oem:{keyword}")

        except Exception as error:
            self.logger.error("Error in hydraulic detection", str(error))

        return {
            'is_relevant': score >= 20,
            'confidence': min(100, score),
            'score': score,
            'indicators': found_indicators,
            'categories': self._categorize_indicators(found_indicators)
        }

    def detect_domain_relevance(self, domain: str) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–∞"""
        score = 0
        found_patterns = []

        try:
            if not domain or not isinstance(domain, str):
                return {'relevant': False, 'score': 0, 'patterns': []}

            domain_lower = domain.lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            for pattern in self.domain_patterns:
                if pattern in domain_lower:
                    score += 15
                    found_patterns.append(pattern)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
            for pattern in self.high_value_domains:
                if pattern in domain_lower:
                    score += 25
                    found_patterns.append(f"high_value:{pattern}")

        except Exception as error:
            self.logger.error("Error checking domain relevance", str(error))

        return {
            'relevant': score >= 15,
            'score': score,
            'patterns': found_patterns
        }

    def _create_empty_detection_result(self) -> Dict[str, Any]:
        return {
            'is_relevant': False,
            'confidence': 0,
            'score': 0,
            'indicators': [],
            'categories': {}
        }

    def _categorize_indicators(self, indicators: List[str]) -> Dict[str, List[str]]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        categories = defaultdict(list)

        for indicator in indicators:
            if ':' in indicator:
                category, value = indicator.split(':', 1)
                categories[category].append(value)
            else:
                categories['general'].append(indicator)

        return dict(categories)

class SmartGeographicPrioritizer:
    """–ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–µ—Ä - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–æ–≥ Google Script"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = SmartFilterLogger("geographic_prioritizer")

        self.high_priority_geo = set(config.get('geographic_priorities', {}).get('high', []))
        self.medium_priority_geo = set(config.get('geographic_priorities', {}).get('medium', []))
        self.regions = set(config.get('geographic_priorities', {}).get('regions', []))

    def get_geographic_priority(self, text: str = "", domain: str = "", email: str = "") -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç"""
        try:
            combined_text = f"{text} {domain} {email}".lower()

            # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            for keyword in self.high_priority_geo:
                if keyword.lower() in combined_text:
                    return {
                        'priority': 'high',
                        'score': 100,
                        'matched_keywords': [keyword],
                        'confidence': 0.9
                    }

            # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            for keyword in self.medium_priority_geo:
                if keyword.lower() in combined_text:
                    return {
                        'priority': 'medium',
                        'score': 60,
                        'matched_keywords': [keyword],
                        'confidence': 0.6
                    }

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤
            for keyword in self.regions:
                if keyword.lower() in combined_text:
                    return {
                        'priority': 'medium',
                        'score': 40,
                        'matched_keywords': [keyword],
                        'confidence': 0.4
                    }

        except Exception as error:
            self.logger.error("Error in geographic prioritization", str(error))

        return {
            'priority': 'low',
            'score': 20,
            'matched_keywords': [],
            'confidence': 0.1
        }

class SmartLeadScorer:
    """–°–∫–æ—Ä–µ—Ä –ª–∏–¥–æ–≤ - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–æ–≥ Google Script"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = SmartFilterLogger("lead_scorer")

        self.weights = config.get('scoring', {}).get('weights', {
            'email_quality': 0.10,
            'company_relevance': 0.45,
            'geographic_priority': 0.30,
            'engagement': 0.15
        })

        self.thresholds = config.get('scoring', {}).get('thresholds', {
            'high_priority': 100,
            'medium_priority': 50,
            'low_priority': 10,
            'exclude': 0
        })

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.hydraulic_detector = SmartHydraulicDetector(config)
        self.geographic_prioritizer = SmartGeographicPrioritizer(config)

    def score_contact(self, email: str, company_name: str = "", description: str = "",
                     title: str = "", domain: str = "", source: str = "") -> Dict[str, Any]:
        """–°–∫–æ—Ä–∏—Ç –∫–æ–Ω—Ç–∞–∫—Ç"""
        try:
            # –ö–∞—á–µ—Å—Ç–≤–æ email
            email_score = self.score_email_quality(email, domain)

            # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏
            relevance_score = self.score_company_relevance(
                company_name, description, title, email, domain
            )

            # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            geo_score = self.score_geographic_priority(
                company_name, description, email, domain
            )

            # –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å
            engagement_score = self.score_engagement(source)

            # –û–±—â–∏–π —Å–∫–æ—Ä
            overall_score = (
                email_score * self.weights['email_quality'] +
                relevance_score * self.weights['company_relevance'] +
                geo_score * self.weights['geographic_priority'] +
                engagement_score * self.weights['engagement']
            )

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            priority = self.get_priority_level(overall_score)
            target_category = self.get_target_category(relevance_score, geo_score)

            return {
                'overall': round(overall_score, 2),
                'breakdown': {
                    'email': email_score,
                    'relevance': relevance_score,
                    'geographic': geo_score,
                    'engagement': engagement_score
                },
                'priority': priority,
                'target_category': target_category,
                'is_qualified': overall_score >= self.thresholds['low_priority']
            }

        except Exception as error:
            self.logger.error("Error scoring contact", str(error))
            return self._create_empty_score()

    def score_email_quality(self, email: str, domain: str = "") -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ email"""
        if not email or '@' not in email:
            return 0

        try:
            score = 25
            email_domain = email.split('@')[1].lower().strip()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
            personal_domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com',
                              'libero.it', 'virgilio.it', 'tim.it']

            if email_domain not in personal_domains:
                score += 35
            else:
                score += 15

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏
            local_part = email.split('@')[0]
            if '.' in local_part:
                score += 15
            if 3 < len(local_part) < 25:
                score += 10

            # –ë–æ–Ω—É—Å –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–º–µ–Ω
            if domain:
                domain_check = self.hydraulic_detector.detect_domain_relevance(domain)
                if domain_check['relevant']:
                    score += 25

            return min(100, score)

        except Exception as error:
            self.logger.error("Error scoring email quality", str(error))
            return 0

    def score_company_relevance(self, company_name: str, description: str, title: str,
                              email: str, domain: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏"""
        try:
            text = f"{company_name} {description} {title}"

            # –î–µ—Ç–µ–∫—Ü–∏—è –≥–∏–¥—Ä–∞–≤–ª–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            hydraulic_check = self.hydraulic_detector.detect_hydraulic_relevance(text)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–∞
            email_domain = email.split('@')[1] if '@' in email else ""
            domain_check = self.hydraulic_detector.detect_domain_relevance(email_domain)
            if domain:
                domain_check_full = self.hydraulic_detector.detect_domain_relevance(domain)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if domain_check_full['score'] > domain_check['score']:
                    domain_check = domain_check_full

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
            base_score = hydraulic_check['score'] + domain_check['score']

            return min(100, max(0, base_score))

        except Exception as error:
            self.logger.error("Error scoring company relevance", str(error))
            return 0

    def score_geographic_priority(self, company_name: str, description: str,
                                email: str, domain: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç"""
        try:
            geo_result = self.geographic_prioritizer.get_geographic_priority(
                f"{company_name} {description}", domain, email
            )
            return geo_result['score']

        except Exception as error:
            self.logger.error("Error scoring geographic priority", str(error))
            return 30

    def score_engagement(self, source: str = "") -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å"""
        try:
            if not source:
                return 25

            source_lower = source.lower()
            if 'contact' in source_lower:
                return 75
            elif 'about' in source_lower:
                return 65
            elif 'product' in source_lower:
                return 85
            elif 'service' in source_lower:
                return 80

            return 40

        except Exception as error:
            self.logger.error("Error scoring engagement", str(error))
            return 25

    def get_priority_level(self, score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞"""
        if score >= self.thresholds['high_priority']:
            return 'high'
        elif score >= self.thresholds['medium_priority']:
            return 'medium'
        elif score >= self.thresholds['low_priority']:
            return 'low'
        return 'exclude'

    def get_target_category(self, relevance_score: float, geo_score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ü–µ–ª–µ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é"""
        if relevance_score >= 70 and geo_score >= 80:
            return 'primary_target'
        elif relevance_score >= 50 and geo_score >= 50:
            return 'secondary_target'
        elif relevance_score >= 30:
            return 'potential'
        return 'excluded'

    def _create_empty_score(self) -> Dict[str, Any]:
        return {
            'overall': 0,
            'breakdown': {'email': 0, 'relevance': 0, 'geographic': 0, 'engagement': 0},
            'priority': 'exclude',
            'target_category': 'excluded',
            'is_qualified': False
        }

class SmartFileProcessor:
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ñ–∞–π–ª–æ–≤ —Å –±—ç–∫–∞–ø–∞–º–∏"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = SmartFilterLogger("file_processor")

        self.backup_dir = Path("backups")
        self.backup_dir.mkdir(exist_ok=True)

        self.processing_config = config.get('processing', {})
        self.backup_original = self.processing_config.get('backup_original', True)

    def create_backup(self, file_path: Path) -> Optional[Path]:
        """–°–æ–∑–¥–∞–µ—Ç –±—ç–∫–∞–ø —Ñ–∞–π–ª–∞"""
        if not self.backup_original:
            return None

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
            backup_path = self.backup_dir / backup_name

            shutil.copy2(file_path, backup_path)
            self.logger.info(f"Backup created: {backup_path}")
            return backup_path

        except Exception as error:
            self.logger.error(f"Failed to create backup for {file_path}", str(error))
            return None

    def atomic_write(self, file_path: Path, data: Any, is_binary: bool = False) -> bool:
        """–ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(
                mode='wb' if is_binary else 'w',
                dir=file_path.parent,
                delete=False,
                encoding=None if is_binary else 'utf-8'
            ) as temp_file:
                temp_path = Path(temp_file.name)

                if is_binary:
                    temp_file.write(data)
                else:
                    if isinstance(data, list):
                        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º CSV/JSON –¥–∞–Ω–Ω—ã–µ
                        if file_path.suffix.lower() == '.json':
                            json.dump(data, temp_file, ensure_ascii=False, indent=2)
                        else:
                            # TXT/CSV —Ñ–æ—Ä–º–∞—Ç
                            if data and isinstance(data[0], list):
                                # CSV-like –¥–∞–Ω–Ω—ã–µ
                                for row in data:
                                    if isinstance(row, list):
                                        temp_file.write('\t'.join(str(x) for x in row) + '\n')
                                    else:
                                        temp_file.write(str(row) + '\n')
                            else:
                                # –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                                for item in data:
                                    temp_file.write(str(item) + '\n')
                    else:
                        temp_file.write(str(data))

            # –ê—Ç–æ–º–∞—Ä–Ω–æ–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
            temp_path.replace(file_path)
            self.logger.info(f"File written successfully: {file_path}")
            return True

        except Exception as error:
            self.logger.error(f"Failed to write file {file_path}", str(error))
            # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            try:
                if 'temp_path' in locals():
                    temp_path.unlink()
            except:
                pass
            return False

    def ensure_directory_exists(self, file_path: Path) -> bool:
        """–£–±–µ–∂–¥–∞–µ—Ç—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        try:
            file_path.parent.mkdir(parents=True, exist_ok=True)
            return True
        except Exception as error:
            self.logger.error(f"Failed to create directory {file_path.parent}", str(error))
            return False

class SmartFilterProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–æ–≥ Google Script"""

    def __init__(self, filter_name: str = "italy_hydraulics"):
        self.filter_name = filter_name
        self.logger = SmartFilterLogger(filter_name)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(filter_name)
        if not self.config:
            raise ValueError(f"Configuration not found: {filter_name}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.exclusion_filter = SmartHardExclusionFilter(self.config)
        self.hydraulic_detector = SmartHydraulicDetector(self.config)
        self.geographic_prioritizer = SmartGeographicPrioritizer(self.config)
        self.lead_scorer = SmartLeadScorer(self.config)
        self.file_processor = SmartFileProcessor(self.config)

        # Load metadata from output directory
        self.metadata_store = self._load_metadata_from_outputs()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.statistics = SmartFilterStatistics()

    def _load_config(self, filter_name: str) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        try:
            config_path = Path("smart_filters") / "configs" / f"{filter_name}.json"
            if not config_path.exists():
                self.logger.error(f"Configuration file not found: {config_path}")
                return None

            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            self.logger.info(f"Configuration loaded: {filter_name}")
            return config

        except Exception as error:
            self.logger.error(f"Failed to load configuration {filter_name}", str(error))
            return None

    def _load_metadata_from_outputs(self) -> Dict[str, Dict]:
        """Load metadata from all *_metadata_*.json files in output/"""
        metadata_store = {}
        output_dir = Path("output")

        if not output_dir.exists():
            self.logger.warning("Output directory not found, metadata will not be loaded")
            return metadata_store

        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤...")
        total_files = 0
        total_records = 0

        for json_file in output_dir.glob("*_metadata_*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    # Check if it's the new format with "metadata" and "emails" keys
                    if isinstance(data, dict) and 'emails' in data:
                        records = data['emails']
                    else:
                        records = data  # Old format: array of records

                    for record in records:
                        email = record.get('email', '').lower()
                        if email:
                            # Only store/update if we have meaningful metadata
                            # Skip records with all null/empty metadata (preserve existing data)
                            page_title = record.get('page_title') or ''
                            meta_desc = record.get('meta_description') or ''
                            category = record.get('category') or ''
                            keywords = record.get('keywords') or ''

                            # If this email already exists and current record has no metadata, skip it
                            if email in metadata_store:
                                existing = metadata_store[email]
                                # Only update if new record has more data than existing
                                if not (page_title or meta_desc or category or keywords):
                                    # New record has no useful data, keep existing
                                    continue

                            metadata_store[email] = {
                                'page_title': page_title,
                                'meta_description': meta_desc,
                                'category': category,
                                'keywords': keywords,
                                'validation_status': record.get('validation_status', 'Valid'),
                                'country': record.get('country', ''),
                                'city': record.get('city', ''),
                                'domain': record.get('domain', '')
                            }

                    total_files += 1
                    total_records += len(records)

            except Exception as e:
                self.logger.error(f"Failed to load metadata from {json_file.name}", str(e))

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ metadata –∏–∑ {total_files} —Ñ–∞–π–ª–æ–≤ –¥–ª—è {len(metadata_store)} emails")
        return metadata_store

    def process_clean_file(self, file_path: Path, include_metadata: bool = True) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç clean —Ñ–∞–π–ª"""
        start_time = time.time()

        try:
            self.logger.info(f"Starting processing: {file_path}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")

            # –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞
            backup_path = self.file_processor.create_backup(file_path)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            data = self._load_file_data(file_path)
            if not data:
                raise ValueError("No data loaded from file")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            processed_results = self._process_data_batch(data)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            output_files = self._save_results(processed_results, file_path, include_metadata)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.statistics.processing_time = time.time() - start_time

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_report(file_path, output_files, backup_path)

            result = {
                'success': True,
                'input_file': str(file_path),
                'output_files': output_files,
                'statistics': asdict(self.statistics),
                'backup_file': str(backup_path) if backup_path else None
            }

            self.logger.info(f"Processing completed: {file_path}")
            return result

        except Exception as error:
            self.logger.error(f"Processing failed: {file_path}", str(error))
            self.statistics.errors += 1

            return {
                'success': False,
                'input_file': str(file_path),
                'error': str(error),
                'statistics': asdict(self.statistics)
            }

    def process_clean_batch(self, pattern: str = "output/*_clean_*.txt") -> List[Dict[str, Any]]:
        """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ clean —Ñ–∞–π–ª–æ–≤"""
        self.logger.info(f"Starting batch processing with pattern: {pattern}")

        results = []

        try:
            # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
            input_files = list(Path(".").glob(pattern))

            if not input_files:
                self.logger.warn(f"No files found matching pattern: {pattern}")
                return results

            self.logger.info(f"Found {len(input_files)} files to process")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
            for file_path in input_files:
                result = self.process_clean_file(file_path, include_metadata=True)
                results.append(result)

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
                time.sleep(self.config.get('processing', {}).get('processing_delay', 50) / 1000)

            self.logger.info(f"Batch processing completed: {len(results)} files processed")
            return results

        except Exception as error:
            self.logger.error(f"Batch processing failed", str(error))
            return results

    def _load_file_data(self, file_path: Path) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]

            self.logger.info(f"Loaded {len(lines)} lines from {file_path}")
            return lines

        except Exception as error:
            self.logger.error(f"Failed to load data from {file_path}", str(error))
            return []

    def _process_data_batch(self, data: List[str]) -> List[SmartFilterResult]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏"""
        batch_size = self.config.get('processing', {}).get('batch_size', 100)
        results = []

        self.statistics.total_processed = len(data)

        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(data) + batch_size - 1) // batch_size

            self.logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} items)")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
            batch_results = self._process_batch(batch)
            results.extend(batch_results)

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + batch_size < len(data):
                delay = self.config.get('processing', {}).get('processing_delay', 50) / 1000
                time.sleep(delay)

        self.logger.info(f"Data processing completed: {len(results)} results")
        return results

    def _process_batch(self, batch: List[str]) -> List[SmartFilterResult]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –±–∞—Ç—á"""
        results = []

        for item in batch:
            try:
                result = self._process_single_item(item)
                if result:
                    results.append(result)
            except Exception as error:
                self.logger.debug(f"Error processing item: {item}", str(error))
                self.statistics.errors += 1

        return results

    def _process_single_item(self, item: str) -> Optional[SmartFilterResult]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ email
            if not self._is_valid_email_format(item):
                return None

            self.statistics.valid_emails += 1

            # Get metadata for this email
            meta = self.metadata_store.get(item.lower(), {})

            # Exclude invalid emails from source data
            if meta.get('validation_status', '').lower() == 'invalid':
                self.statistics.hard_excluded += 1
                self.statistics.excluded_by_category['invalid_status'] += 1
                return None

            # –ñ–µ—Å—Ç–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            exclusion_result = self.exclusion_filter.should_exclude(
                item,
                company_name=meta.get('page_title', ''),
                description=meta.get('meta_description', ''),
                domain=meta.get('domain', '')
            )
            if exclusion_result['should_exclude']:
                self.statistics.hard_excluded += 1
                for reason in exclusion_result['reasons']:
                    self.statistics.excluded_by_category[reason] += 1
                return None

            # –°–∫–æ—Ä–∏–Ω–≥ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            scoring_result = self.lead_scorer.score_contact(
                email=item,
                company_name=meta.get('page_title', ''),        # ‚úÖ Company name from title
                description=meta.get('meta_description', ''),   # ‚úÖ Description (MAIN!)
                title=meta.get('category', ''),                 # ‚úÖ Business category
                domain=meta.get('domain', '') or (item.split('@')[1] if '@' in item else ""),
                source=meta.get('keywords', '')                 # ‚úÖ Search keywords
            )

            if not scoring_result['is_qualified']:
                return None

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.statistics.qualified_leads += 1
            if scoring_result['priority'] == 'high':
                self.statistics.high_priority += 1
            elif scoring_result['priority'] == 'medium':
                self.statistics.medium_priority += 1
            elif scoring_result['priority'] == 'low':
                self.statistics.low_priority += 1

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = SmartFilterResult(
                email=item,
                original_score=0,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                final_score=scoring_result['overall'],
                priority=scoring_result['priority'],
                target_category=scoring_result['target_category'],
                exclusion_reasons=[],
                indicators={
                    'scoring_breakdown': scoring_result['breakdown'],
                    'hydraulic_detection': {},  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    'geographic_priority': {}   # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                },
                metadata={
                    'filter_name': self.filter_name,
                    'config_version': self.config.get('version', '1.0'),
                    'processing_timestamp': datetime.now().isoformat()
                },
                processing_timestamp=datetime.now().isoformat()
            )

            return result

        except Exception as error:
            self.logger.error(f"Error processing item: {item}", str(error))
            self.statistics.errors += 1
            return None

    def _is_valid_email_format(self, email: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç email"""
        try:
            pattern = r'^[a-zA-Z0-9.!#$%&\'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$'
            return bool(re.match(pattern, email)) and len(email) <= 254
        except:
            return False

    def _save_results(self, results: List[SmartFilterResult], original_file: Path,
                     include_metadata: bool) -> Dict[str, str]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        output_files = {}

        try:
            output_config = self.config.get('output', {})
            prefix = output_config.get('prefix', 'smart_filtered_')
            base_name = f"{prefix}{original_file.stem}"
            output_dir = original_file.parent

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            qualified_results = [r for r in results if r.priority != 'exclude']

            if not qualified_results:
                self.logger.warn("No qualified results to save")
                return output_files

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ email (TXT —Ñ–æ—Ä–º–∞—Ç)
            txt_file = output_dir / f"{base_name}.txt"
            txt_data = [r.email for r in qualified_results]
            if self.file_processor.atomic_write(txt_file, txt_data):
                output_files['txt'] = str(txt_file)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (CSV —Ñ–æ—Ä–º–∞—Ç)
            if include_metadata:
                csv_file = output_dir / f"{base_name}.csv"
                csv_data = self._prepare_csv_data(qualified_results)
                if self.file_processor.atomic_write(csv_file, csv_data):
                    output_files['csv'] = str(csv_file)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
                json_file = output_dir / f"{base_name}.json"
                json_data = [asdict(r) for r in qualified_results]
                if self.file_processor.atomic_write(json_file, json_data):
                    output_files['json'] = str(json_file)

            self.logger.info(f"Results saved: {len(output_files)} files")
            return output_files

        except Exception as error:
            self.logger.error("Failed to save results", str(error))
            return output_files

    def _prepare_csv_data(self, results: List[SmartFilterResult]) -> List[List[str]]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç CSV –¥–∞–Ω–Ω—ã–µ"""
        try:
            headers = [
                'email', 'final_score', 'priority', 'target_category',
                'processing_timestamp', 'filter_name'
            ]

            data = [headers]

            for result in results:
                row = [
                    result.email,
                    str(result.final_score),
                    result.priority,
                    result.target_category,
                    result.processing_timestamp,
                    result.metadata.get('filter_name', '')
                ]
                data.append(row)

            return data

        except Exception as error:
            self.logger.error("Failed to prepare CSV data", str(error))
            return []

    def _generate_report(self, input_file: Path, output_files: Dict[str, str],
                        backup_file: Optional[Path]):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
        try:
            report_dir = Path("reports")
            report_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"{self.filter_name}_report_{timestamp}.txt"

            report_content = self._generate_report_content(
                input_file, output_files, backup_file
            )

            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            self.logger.info(f"Report generated: {report_file}")

        except Exception as error:
            self.logger.error("Failed to generate report", str(error))

    def _generate_report_content(self, input_file: Path, output_files: Dict[str, str],
                                backup_file: Optional[Path]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç—á–µ—Ç–∞"""
        lines = [
            "=" * 80,
            f"SMART FILTER PROCESSING REPORT",
            "=" * 80,
            f"Filter: {self.filter_name}",
            f"Config: {self.config.get('display_name', 'Unknown')}",
            f"Input file: {input_file}",
            f"Backup file: {backup_file if backup_file else 'N/A'}",
            f"Processing time: {self.statistics.processing_time:.2f} seconds",
            "",
            "STATISTICS:",
            f"  Total processed: {self.statistics.total_processed}",
            f"  Valid emails: {self.statistics.valid_emails}",
            f"  Hard excluded: {self.statistics.hard_excluded}",
            f"  Qualified leads: {self.statistics.qualified_leads}",
            f"  High priority: {self.statistics.high_priority}",
            f"  Medium priority: {self.statistics.medium_priority}",
            f"  Low priority: {self.statistics.low_priority}",
            f"  Errors: {self.statistics.errors}",
            "",
            "EXCLUSION REASONS:",
        ]

        for reason, count in self.statistics.excluded_by_category.items():
            lines.append(f"  {reason}: {count}")

        lines.extend([
            "",
            "OUTPUT FILES:",
        ])

        for file_type, file_path in output_files.items():
            lines.append(f"  {file_type.upper()}: {file_path}")

        lines.extend([
            "",
            "=" * 80,
            f"Generated: {datetime.now().isoformat()}",
            "=" * 80
        ])

        return '\n'.join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        return asdict(self.statistics)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

def get_available_configs() -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
    try:
        configs_dir = Path("configs")
        if not configs_dir.exists():
            return []

        configs = []
        for config_file in configs_dir.glob("*.json"):
            configs.append(config_file.stem)

        return sorted(configs)

    except Exception:
        return []

def load_config_preview(config_name: str) -> Optional[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    try:
        config_path = Path("configs") / f"{config_name}.json"
        if not config_path.exists():
            return None

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
        return {
            'config_name': config.get('config_name'),
            'display_name': config.get('display_name'),
            'version': config.get('version'),
            'target_market': config.get('target_market'),
            'target_industry': config.get('target_industry')
        }

    except Exception:
        return None

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import sys

    if len(sys.argv) > 1:
        config_name = sys.argv[1]
        file_path = sys.argv[2] if len(sys.argv) > 2 else None

        if file_path:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            processor = SmartFilterProcessor(config_name)
            result = processor.process_clean_file(Path(file_path))
            print(f"Result: {result}")
        else:
            # –ü–æ–∫–∞–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
            configs = get_available_configs()
            print(f"Available configs: {configs}")
    else:
        print("Usage: python3 smart_filter_processor_v2.py <config_name> [file_path]")