#!/usr/bin/env python3
"""
–ú–∏–≥—Ä–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö processing_cache_optimized.db –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—à–±–æ—Ä–¥–∞.

–î–æ–±–∞–≤–ª—è–µ—Ç:
1. –¢–∞–±–ª–∏—Ü—É processing_statistics - –æ–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (single-row)
2. –¢–∞–±–ª–∏—Ü—É country_statistics - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
3. –¢–∞–±–ª–∏—Ü—É processing_history - –∏—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
4. –ö–æ–ª–æ–Ω–∫–∏ country, category –≤ processed_files

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 migrate_statistics_db.py
"""

import sqlite3
import json
from pathlib import Path
from datetime import datetime
import sys


def create_backup(db_path):
    """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    backup_path = db_path.with_suffix('.db.backup')

    if backup_path.exists():
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = db_path.parent / f"{db_path.stem}_{timestamp}.db.backup"

    try:
        import shutil
        shutil.copy2(db_path, backup_path)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path}")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
        return False


def check_database_exists(db_path):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    if not db_path.exists():
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã:")
        print("   python3 email_checker.py incremental --exclude-duplicates")
        return False
    return True


def create_new_tables(cursor):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü...")

    # –¢–∞–±–ª–∏—Ü–∞ 1: –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (single-row)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processing_statistics (
            id INTEGER PRIMARY KEY CHECK (id = 1),

            -- –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_lists INTEGER DEFAULT 0,
            total_processed_emails INTEGER DEFAULT 0,
            total_clean_emails INTEGER DEFAULT 0,
            total_blocked_emails INTEGER DEFAULT 0,
            total_invalid_emails INTEGER DEFAULT 0,

            -- –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            last_updated TEXT NOT NULL,
            calculated_at TEXT NOT NULL,

            -- JSON –ø–æ–ª—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            countries_json TEXT,
            categories_json TEXT,

            -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            version INTEGER DEFAULT 1,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_stats_last_updated
        ON processing_statistics(last_updated)
    ''')

    print("  ‚úì –¢–∞–±–ª–∏—Ü–∞ processing_statistics —Å–æ–∑–¥–∞–Ω–∞")

    # –¢–∞–±–ª–∏—Ü–∞ 2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS country_statistics (
            country TEXT PRIMARY KEY,

            -- –°—á–µ—Ç—á–∏–∫–∏
            total_lists INTEGER DEFAULT 0,
            clean_emails INTEGER DEFAULT 0,
            blocked_emails INTEGER DEFAULT 0,
            total_emails INTEGER DEFAULT 0,

            -- –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_score REAL DEFAULT 0.0,

            -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            last_updated TEXT NOT NULL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_country_quality
        ON country_statistics(quality_score DESC)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_country_total
        ON country_statistics(total_emails DESC)
    ''')

    print("  ‚úì –¢–∞–±–ª–∏—Ü–∞ country_statistics —Å–æ–∑–¥–∞–Ω–∞")

    # –¢–∞–±–ª–∏—Ü–∞ 3: –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processing_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,

            -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
            filename TEXT NOT NULL,
            file_path TEXT NOT NULL,
            file_type TEXT NOT NULL,

            -- –î–µ—Ç–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            processed_at TEXT NOT NULL,
            processing_time REAL DEFAULT 0.0,
            success INTEGER DEFAULT 1,

            -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_emails INTEGER DEFAULT 0,
            clean_emails INTEGER DEFAULT 0,
            blocked_emails INTEGER DEFAULT 0,
            invalid_emails INTEGER DEFAULT 0,

            -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            country TEXT,
            category TEXT,

            -- –î–ª—è recent activity feed
            output_size INTEGER DEFAULT 0,

            -- –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_history_processed_at
        ON processing_history(processed_at DESC)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_history_success
        ON processing_history(success)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_history_country
        ON processing_history(country)
    ''')

    print("  ‚úì –¢–∞–±–ª–∏—Ü–∞ processing_history —Å–æ–∑–¥–∞–Ω–∞")


def add_missing_columns(cursor):
    """–î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ processed_files"""
    print("\nüîß –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã processed_files...")

    # –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
    cursor.execute("PRAGMA table_info(processed_files)")
    existing_columns = {row[1] for row in cursor.fetchall()}

    columns_to_add = {
        'country': 'TEXT',
        'category': 'TEXT',
        'file_type': 'TEXT',
        'success': 'INTEGER DEFAULT 1',
        'processing_time': 'REAL DEFAULT 0.0',
        'error': 'TEXT',
        'duplicates_removed': 'INTEGER DEFAULT 0',
        'prefix_duplicates_removed': 'INTEGER DEFAULT 0',
        'has_metadata': 'INTEGER DEFAULT 0'
    }

    added_count = 0
    for column_name, column_type in columns_to_add.items():
        if column_name not in existing_columns:
            try:
                cursor.execute(f"ALTER TABLE processed_files ADD COLUMN {column_name} {column_type}")
                print(f"  ‚úì –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {column_name}")
                added_count += 1
            except sqlite3.OperationalError as e:
                if "duplicate column name" not in str(e).lower():
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏ {column_name}: {e}")

    if added_count == 0:
        print("  ‚úì –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç")
    else:
        print(f"  ‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {added_count}")


def load_lists_config():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–ø–∏—Å–∫–æ–≤"""
    config_path = Path("lists_config.json")

    if not config_path.exists():
        print("  ‚ö†Ô∏è  –§–∞–π–ª lists_config.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return {}

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç: –º–æ–∂–µ—Ç –±—ã—Ç—å {"lists": [...]} –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ [...]
            if isinstance(data, dict) and 'lists' in data:
                lists = data['lists']
            elif isinstance(data, list):
                lists = data
            else:
                print(f"  ‚ö†Ô∏è  –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç lists_config.json")
                return {}

            return {item['filename']: item for item in lists}
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è lists_config.json: {e}")
        import traceback
        print(traceback.format_exc())
        return {}


def backfill_country_category(cursor, lists_config):
    """–ó–∞–ø–æ–ª–Ω–∏—Ç—å —Å—Ç—Ä–∞–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ lists_config.json"""
    print("\nüåç –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")

    if not lists_config:
        print("  ‚ö†Ô∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–æ–≤ –ø—É—Å—Ç–∞, –ø—Ä–æ–ø—É—Å–∫")
        return

    cursor.execute("SELECT file_path FROM processed_files")
    files = cursor.fetchall()

    updated_count = 0
    for (file_path,) in files:
        filename = Path(file_path).name

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if filename in lists_config:
            config = lists_config[filename]
            country = config.get('country', 'Unknown')
            category = config.get('category', 'Other')
        else:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É (–±–µ–∑ –¥–∞—Ç—ã)
            base_name = filename.split('_clean_')[0].split('_blocked_')[0]

            country = 'Unknown'
            category = 'Other'

            for config_filename, config in lists_config.items():
                if config_filename.startswith(base_name):
                    country = config.get('country', 'Unknown')
                    category = config.get('category', 'Other')
                    break

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ñ–∞–π–ª–∞
        file_type = 'lvp' if file_path.lower().endswith('.lvp') else 'txt'

        cursor.execute('''
            UPDATE processed_files
            SET country = ?, category = ?, file_type = ?
            WHERE file_path = ?
        ''', (country, category, file_type, file_path))

        updated_count += 1

    print(f"  ‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {updated_count}")


def calculate_initial_statistics(cursor, conn):
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüìà –†–∞—Å—á–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")

    # 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    cursor.execute('''
        SELECT
            COUNT(DISTINCT file_path) as total_lists,
            COALESCE(SUM(total_count), 0) as total_emails,
            COALESCE(SUM(clean_count), 0) as clean,
            COALESCE(SUM(blocked_count), 0) as blocked,
            COALESCE(SUM(invalid_count), 0) as invalid
        FROM processed_files
        WHERE file_hash IS NOT NULL
    ''')

    row = cursor.fetchone()
    total_lists, total_emails, clean, blocked, invalid = row

    # –ü–æ–ª—É—á–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    cursor.execute('SELECT DISTINCT country FROM processed_files WHERE country IS NOT NULL')
    countries = [r[0] for r in cursor.fetchall()]

    cursor.execute('''
        SELECT category, COUNT(*) as count
        FROM processed_files
        WHERE category IS NOT NULL
        GROUP BY category
    ''')
    categories = {row[0]: row[1] for row in cursor.fetchall()}

    now = datetime.now().isoformat()

    cursor.execute('''
        INSERT OR REPLACE INTO processing_statistics
        (id, total_lists, total_processed_emails, total_clean_emails,
         total_blocked_emails, total_invalid_emails,
         countries_json, categories_json,
         last_updated, calculated_at)
        VALUES (1, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        total_lists,
        total_emails,
        clean,
        blocked,
        invalid,
        json.dumps(countries, ensure_ascii=False),
        json.dumps(categories, ensure_ascii=False),
        now,
        now
    ))

    print(f"  ‚úì –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"    - –°–ø–∏—Å–∫–æ–≤: {total_lists}")
    print(f"    - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_emails:,}")
    print(f"    - –ß–∏—Å—Ç—ã—Ö: {clean:,}")
    print(f"    - –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {blocked:,}")
    print(f"    - –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö: {invalid:,}")

    # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
    cursor.execute('''
        SELECT
            country,
            COUNT(*) as lists,
            COALESCE(SUM(clean_count), 0) as clean,
            COALESCE(SUM(blocked_count), 0) as blocked,
            COALESCE(SUM(total_count), 0) as total
        FROM processed_files
        WHERE country IS NOT NULL AND country != 'Unknown'
        GROUP BY country
    ''')

    country_count = 0
    for row in cursor.fetchall():
        country, lists, clean_cnt, blocked_cnt, total_cnt = row
        quality = (clean_cnt / total_cnt * 100) if total_cnt > 0 else 0.0

        cursor.execute('''
            INSERT OR REPLACE INTO country_statistics
            (country, total_lists, clean_emails, blocked_emails,
             total_emails, quality_score, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (country, lists, clean_cnt, blocked_cnt, total_cnt, quality, now))

        country_count += 1

    print(f"  ‚úì –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º: {country_count} —Å—Ç—Ä–∞–Ω")

    # 3. –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    cursor.execute('''
        SELECT file_path, processed_at,
               COALESCE(total_count, 0), COALESCE(clean_count, 0),
               COALESCE(blocked_count, 0), COALESCE(invalid_count, 0),
               country, category, file_type
        FROM processed_files
        WHERE processed_at IS NOT NULL
        ORDER BY processed_at DESC
    ''')

    history_count = 0
    for row in cursor.fetchall():
        file_path, processed_at, total, clean_cnt, blocked_cnt, invalid_cnt, country, category, file_type = row
        filename = Path(file_path).name

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—å
        cursor.execute('''
            SELECT COUNT(*) FROM processing_history
            WHERE file_path = ? AND processed_at = ?
        ''', (file_path, processed_at))

        if cursor.fetchone()[0] == 0:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä output —Ñ–∞–π–ª–∞
            output_size = 0
            try:
                output_dir = Path("output")
                stem = Path(file_path).stem
                clean_files = list(output_dir.glob(f"{stem}_clean_*.txt"))
                if clean_files:
                    latest_file = max(clean_files, key=lambda f: f.stat().st_mtime)
                    output_size = latest_file.stat().st_size
                else:
                    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: ~50 –±–∞–π—Ç –Ω–∞ email
                    output_size = clean_cnt * 50
            except:
                output_size = clean_cnt * 50

            cursor.execute('''
                INSERT INTO processing_history
                (filename, file_path, file_type, processed_at,
                 total_emails, clean_emails, blocked_emails, invalid_emails,
                 country, category, success, output_size)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, ?)
            ''', (
                filename,
                file_path,
                file_type or 'txt',
                processed_at,
                total,
                clean_cnt,
                blocked_cnt,
                invalid_cnt,
                country,
                category,
                output_size
            ))
            history_count += 1

    print(f"  ‚úì –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {history_count} –∑–∞–ø–∏—Å–µ–π")

    conn.commit()


def verify_migration(cursor):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏"""
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏...")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–∞–±–ª–∏—Ü—ã
    cursor.execute("""
        SELECT name FROM sqlite_master
        WHERE type='table' AND name IN (
            'processing_statistics',
            'country_statistics',
            'processing_history'
        )
    """)

    tables = [row[0] for row in cursor.fetchall()]

    required_tables = ['processing_statistics', 'country_statistics', 'processing_history']
    missing_tables = set(required_tables) - set(tables)

    if missing_tables:
        print(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–±–ª–∏—Ü—ã: {missing_tables}")
        return False

    print(f"  ‚úì –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã: {len(tables)}")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
    cursor.execute("SELECT COUNT(*) FROM processing_statistics")
    stats_count = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM country_statistics")
    country_count = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM processing_history")
    history_count = cursor.fetchone()[0]

    print(f"  ‚úì –î–∞–Ω–Ω—ã–µ –≤ processing_statistics: {stats_count} —Å—Ç—Ä–æ–∫")
    print(f"  ‚úì –î–∞–Ω–Ω—ã–µ –≤ country_statistics: {country_count} —Å—Ç—Ä–∞–Ω")
    print(f"  ‚úì –î–∞–Ω–Ω—ã–µ –≤ processing_history: {history_count} –∑–∞–ø–∏—Å–µ–π")

    if stats_count == 0:
        print("  ‚ö†Ô∏è  processing_statistics –ø—É—Å—Ç–∞ (–Ω–æ—Ä–º–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤)")

    return True


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏"""
    print("=" * 70)
    print("üîÑ –ú–ò–ì–†–ê–¶–ò–Ø –ë–î: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞")
    print("=" * 70)

    # –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    db_path = Path(".cache/processing_cache_optimized.db")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
    if not check_database_exists(db_path):
        return False

    # –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
    if not create_backup(db_path):
        response = input("‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏? (y/N): ")
        if response.lower() != 'y':
            print("‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
            return False

    try:
        # –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # –®–∞–≥ 1: –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        create_new_tables(cursor)
        conn.commit()

        # –®–∞–≥ 2: –î–æ–±–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –≤ processed_files
        add_missing_columns(cursor)
        conn.commit()

        # –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        lists_config = load_lists_config()

        # –®–∞–≥ 4: –ó–∞–ø–æ–ª–Ω–∏—Ç—å —Å—Ç—Ä–∞–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        backfill_country_category(cursor, lists_config)
        conn.commit()

        # –®–∞–≥ 5: –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        calculate_initial_statistics(cursor, conn)

        # –®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é
        if not verify_migration(cursor):
            print("\n‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
            conn.close()
            return False

        conn.close()

        print("\n" + "=" * 70)
        print("‚úÖ –ú–ò–ì–†–ê–¶–ò–Ø –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 70)
        print("\n–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: python3 validate_statistics.py")
        print("2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä: python3 web_server.py")
        print("3. –û—Ç–∫—Ä–æ–π—Ç–µ –¥–∞—à–±–æ—Ä–¥: http://localhost:8080/new#dashboard")
        print("\n–í —Å–ª—É—á–∞–µ –ø—Ä–æ–±–ª–µ–º –º–æ–∂–Ω–æ –æ—Ç–∫–∞—Ç–∏—Ç—å—Å—è –∫ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏.")

        return True

    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –ú–ò–ì–†–ê–¶–ò–ò: {e}")
        import traceback
        print(traceback.format_exc())

        if 'conn' in locals():
            conn.rollback()
            conn.close()

        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
