#!/usr/bin/env python3
"""
Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² ÐºÐµÑˆÐ° Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°
"""

import os
import json
from pathlib import Path
from datetime import datetime, timedelta
import shutil

def cleanup_cache_backups(cache_dir=".cache", keep_latest=1):
    """
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ñ… ÐºÐ¾Ð¿Ð¸Ð¹ JSON ÐºÐµÑˆÐ°

    Args:
        cache_dir: Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ ÐºÐµÑˆÐ°
        keep_latest: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… ÐºÐ¾Ð¿Ð¸Ð¹ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ

    Returns:
        dict: Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
    """
    cache_path = Path(cache_dir)
    stats = {
        "deleted_files": [],
        "freed_space_mb": 0,
        "kept_files": [],
        "errors": []
    }

    if not cache_path.exists():
        print(f"âŒ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ {cache_dir} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
        return stats

    # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð²ÑÐµ backup Ñ„Ð°Ð¹Ð»Ñ‹
    backup_files = list(cache_path.glob("processed_files_backup_*.json"))

    if not backup_files:
        print("âœ… Ð ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¿Ð¸Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹")
        return stats

    # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¼Ð¾Ð´Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ (Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€Ð²Ñ‹Ðµ)
    backup_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)

    print(f"\nðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ñ… ÐºÐ¾Ð¿Ð¸Ð¹: {len(backup_files)}")
    total_size = sum(f.stat().st_size for f in backup_files) / (1024 * 1024)
    print(f"ðŸ“¦ ÐžÐ±Ñ‰Ð¸Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€: {total_size:.2f} MB")

    # ÐžÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ N ÐºÐ¾Ð¿Ð¸Ð¹
    files_to_keep = backup_files[:keep_latest]
    files_to_delete = backup_files[keep_latest:]

    print(f"\nðŸ—‘ï¸  Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ: {len(files_to_delete)}")
    print(f"ðŸ’¾ Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ: {len(files_to_keep)}")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÐºÐ¾Ð¿Ð¸Ð¸
    for file in files_to_keep:
        size_mb = file.stat().st_size / (1024 * 1024)
        stats["kept_files"].append(f"{file.name} ({size_mb:.2f} MB)")
        print(f"  âœ“ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼: {file.name} ({size_mb:.2f} MB)")

    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ ÐºÐ¾Ð¿Ð¸Ð¸
    for file in files_to_delete:
        try:
            size_mb = file.stat().st_size / (1024 * 1024)
            file.unlink()
            stats["deleted_files"].append(file.name)
            stats["freed_space_mb"] += size_mb
            print(f"  ðŸ—‘ï¸ Ð£Ð´Ð°Ð»ÐµÐ½: {file.name} ({size_mb:.2f} MB)")
        except Exception as e:
            stats["errors"].append(f"{file.name}: {str(e)}")
            print(f"  âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸ {file.name}: {e}")

    return stats

def cleanup_duplicate_db_files(cache_dir=".cache"):
    """
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ…ÑÑ Ð¸Ð»Ð¸ Ð½ÐµÐ½ÑƒÐ¶Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð‘Ð”

    Returns:
        dict: Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
    """
    cache_path = Path(cache_dir)
    stats = {
        "deleted_files": [],
        "freed_space_mb": 0,
        "errors": []
    }

    # Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
    db_files_to_check = [
        "processing_cache.db",  # Ð¡Ñ‚Ð°Ñ€Ð°Ñ Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð‘Ð”
        "test_cache.db"  # Ð¢ÐµÑÑ‚Ð¾Ð²Ð°Ñ Ð‘Ð”
    ]

    for db_file in db_files_to_check:
        file_path = cache_path / db_file
        if file_path.exists():
            try:
                size_mb = file_path.stat().st_size / (1024 * 1024)

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð»Ð¸ Ñ„Ð°Ð¹Ð»
                if db_file == "processing_cache.db":
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ
                    final_db = cache_path / "processing_cache_final.db"
                    if final_db.exists():
                        print(f"\nðŸ” ÐÐ°Ð¹Ð´ÐµÐ½ ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ð¹ {db_file} ({size_mb:.2f} MB)")
                        print("   Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ processing_cache_final.db, ÑƒÐ´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ...")
                        file_path.unlink()
                        stats["deleted_files"].append(db_file)
                        stats["freed_space_mb"] += size_mb
                        print(f"   âœ“ Ð£Ð´Ð°Ð»ÐµÐ½: {db_file}")
                elif db_file == "test_cache.db":
                    print(f"\nðŸ—‘ï¸  Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²ÑƒÑŽ Ð‘Ð”: {db_file} ({size_mb:.2f} MB)")
                    file_path.unlink()
                    stats["deleted_files"].append(db_file)
                    stats["freed_space_mb"] += size_mb
                    print(f"   âœ“ Ð£Ð´Ð°Ð»ÐµÐ½: {db_file}")

            except PermissionError:
                stats["errors"].append(f"{db_file}: Ñ„Ð°Ð¹Ð» Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ")
                print(f"   âš ï¸  {db_file} Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð¼, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼")
            except Exception as e:
                stats["errors"].append(f"{db_file}: {str(e)}")
                print(f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸ {db_file}: {e}")

    return stats

def cleanup_output_duplicates(output_dir="output", pattern="*_metadata_*.json"):
    """
    Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ…ÑÑ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²

    Args:
        output_dir: Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ð¼Ð¸ Ñ„Ð°Ð¹Ð»Ð°Ð¼Ð¸
        pattern: Ð¨Ð°Ð±Ð»Ð¾Ð½ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²

    Returns:
        dict: Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
    """
    output_path = Path(output_dir)
    stats = {
        "deleted_files": 0,
        "freed_space_mb": 0,
        "groups_processed": 0
    }

    if not output_path.exists():
        print(f"âŒ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ {output_dir} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
        return stats

    # Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¿Ð¾ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¼Ñƒ Ð¸Ð¼ÐµÐ½Ð¸
    file_groups = {}
    for file in output_path.glob(pattern):
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ð¾Ðµ Ð¸Ð¼Ñ (Ð±ÐµÐ· timestamp)
        base_name = "_".join(file.stem.split("_")[:-2])  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð¸ Ð²Ñ€ÐµÐ¼Ñ
        if base_name not in file_groups:
            file_groups[base_name] = []
        file_groups[base_name].append(file)

    print(f"\nðŸ“‚ ÐÐ½Ð°Ð»Ð¸Ð· Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð² {output_dir}:")
    print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð³Ñ€ÑƒÐ¿Ð¿ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(file_groups)}")

    for base_name, files in file_groups.items():
        if len(files) > 1:
            stats["groups_processed"] += 1
            # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¼Ð¾Ð´Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ (Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€Ð²Ñ‹Ðµ)
            files.sort(key=lambda f: f.stat().st_mtime, reverse=True)

            # ÐžÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ°Ð¼Ñ‹Ð¹ Ð½Ð¾Ð²Ñ‹Ð¹
            newest = files[0]
            duplicates = files[1:]

            if duplicates:
                print(f"\n   ðŸ“„ {base_name}:")
                print(f"      Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼: {newest.name}")
                for dup in duplicates:
                    size_mb = dup.stat().st_size / (1024 * 1024)
                    try:
                        dup.unlink()
                        stats["deleted_files"] += 1
                        stats["freed_space_mb"] += size_mb
                        print(f"      ðŸ—‘ï¸ Ð£Ð´Ð°Ð»ÐµÐ½ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚: {dup.name} ({size_mb:.2f} MB)")
                    except Exception as e:
                        print(f"      âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸ {dup.name}: {e}")

    return stats

def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            ÐžÐ§Ð˜Ð¡Ð¢ÐšÐ Ð˜Ð—Ð‘Ð«Ð¢ÐžÐ§ÐÐ«Ð¥ Ð¤ÐÐ™Ð›ÐžÐ’                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    total_freed = 0

    # 1. ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ñ… ÐºÐ¾Ð¿Ð¸Ð¹ ÐºÐµÑˆÐ°
    print("\n1ï¸âƒ£  ÐžÐ§Ð˜Ð¡Ð¢ÐšÐ Ð Ð•Ð—Ð•Ð Ð’ÐÐ«Ð¥ ÐšÐžÐŸÐ˜Ð™ ÐšÐ•Ð¨Ð")
    print("=" * 60)
    cache_stats = cleanup_cache_backups(keep_latest=1)
    total_freed += cache_stats["freed_space_mb"]

    # 2. ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ…ÑÑ Ð‘Ð”
    print("\n2ï¸âƒ£  ÐžÐ§Ð˜Ð¡Ð¢ÐšÐ Ð”Ð£Ð‘Ð›Ð˜Ð Ð£Ð®Ð©Ð˜Ð¥Ð¡Ð¯ Ð‘Ð”")
    print("=" * 60)
    db_stats = cleanup_duplicate_db_files()
    total_freed += db_stats["freed_space_mb"]

    # 3. ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð² output (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
    print("\n3ï¸âƒ£  ÐÐÐÐ›Ð˜Ð— Ð”Ð£Ð‘Ð›Ð˜ÐšÐÐ¢ÐžÐ’ Ð’ OUTPUT")
    print("=" * 60)
    response = input("Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ metadata Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² output/? (y/N): ")
    if response.lower() == 'y':
        output_stats = cleanup_output_duplicates()
        total_freed += output_stats["freed_space_mb"]
    else:
        print("   ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾ Ð¿Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ")

    # Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    print("\n" + "=" * 60)
    print("ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’ÐÐ¯ Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ:")
    print("=" * 60)
    print(f"âœ… Ð’ÑÐµÐ³Ð¾ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¾: {total_freed:.2f} MB")
    print(f"ðŸ—‘ï¸  Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(cache_stats['deleted_files']) + len(db_stats['deleted_files'])}")

    if cache_stats["errors"] or db_stats["errors"]:
        print(f"\nâš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ¸ Ð¿Ñ€Ð¸ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐµ:")
        for error in cache_stats["errors"] + db_stats["errors"]:
            print(f"   - {error}")

    print("\nâœ¨ ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")

    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¸ÑÐºÐ°
    print("\nðŸ“¦ Ð¢Ð•ÐšÐ£Ð©Ð•Ð• Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐÐ˜Ð• Ð”Ð˜Ð¡ÐšÐ:")
    print("=" * 60)
    dirs_to_check = [".cache", "output", "input"]
    for dir_name in dirs_to_check:
        dir_path = Path(dir_name)
        if dir_path.exists():
            total_size = sum(f.stat().st_size for f in dir_path.rglob("*") if f.is_file())
            print(f"  {dir_name:20s}: {total_size / (1024**2):>10.2f} MB")

if __name__ == "__main__":
    main()