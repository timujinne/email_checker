#!/usr/bin/env python3
"""
Web server –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è email —Å–ø–∏—Å–∫–∞–º–∏ - –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –í–ï–†–°–ò–Ø
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ Command Injection
"""

import os
import sys
import json
import subprocess
import shlex  # –î–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥
from http.server import BaseHTTPRequestHandler, HTTPServer
from pathlib import Path
from urllib.parse import parse_qs, urlparse
from datetime import datetime
import threading
import time
import sqlite3
import hashlib

# –ò–º–ø–æ—Ä—Ç Blocklist API
from blocklist_api import (
    handle_get_blocklist,
    handle_get_blocklist_stats,
    handle_blocklist_add,
    handle_blocklist_remove,
    handle_blocklist_bulk_add,
    handle_blocklist_bulk_remove,
    handle_blocklist_import_csv,
    handle_blocklist_search,
    handle_blocklist_export
)

# –ò–º–ø–æ—Ä—Ç Email Records API
from email_records_api import (
    handle_get_emails,
    handle_get_email_count,
    handle_get_single_email,
    handle_bulk_update_emails,
    handle_bulk_delete_emails,
    handle_export_emails,
    handle_delete_email,
    handle_bulk_status_update
)

# –ò–º–ø–æ—Ä—Ç WebSocket —Å–µ—Ä–≤–µ—Ä–∞
import websocket_server

# –ò–º–ø–æ—Ä—Ç EmailChecker –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
from email_checker import EmailChecker

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
processing_state = {
    "is_running": False,
    "logs": [],
    "lock": threading.Lock(),
    "start_time": None,
    "processed_files": 0,
    "total_files": 0
}

# –ë–ï–õ–´–ô –°–ü–ò–°–û–ö —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
ALLOWED_COMMANDS = {
    "check", "check-lvp", "check-enriched", "check-metadata",
    "check-sequence", "check-lvp-sequence", "batch", "check-lvp-batch",
    "check-all-incremental", "incremental", "status", "report"
}

# –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –¥–ª–∏–Ω–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∞—Ç–∞–∫
MAX_FILENAME_LENGTH = 255

def validate_filename(filename):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è path traversal –∏ injection –∞—Ç–∞–∫

    Args:
        filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

    Returns:
        bool: True –µ—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ

    Raises:
        ValueError: –ï—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø–∞—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
    if len(filename) > MAX_FILENAME_LENGTH:
        raise ValueError(f"Filename too long: {len(filename)} > {MAX_FILENAME_LENGTH}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ path traversal
    if ".." in filename or "/" in filename or "\\" in filename:
        raise ValueError(f"Path traversal attempt detected in filename: {filename}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–ø–∞—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    dangerous_chars = [";", "&", "|", "`", "$", "(", ")", "{", "}", "[", "]", "<", ">", "!", "\\n", "\\r"]
    for char in dangerous_chars:
        if char in filename:
            raise ValueError(f"Dangerous character '{char}' in filename: {filename}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    allowed_extensions = {".txt", ".lvp", ".csv", ".json"}
    file_path = Path(filename)
    if file_path.suffix.lower() not in allowed_extensions:
        raise ValueError(f"Invalid file extension: {file_path.suffix}")

    return True

def validate_command(command):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è injection –∞—Ç–∞–∫

    Args:
        command: –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

    Returns:
        bool: True –µ—Å–ª–∏ –∫–æ–º–∞–Ω–¥–∞ –±–µ–∑–æ–ø–∞—Å–Ω–∞

    Raises:
        ValueError: –ï—Å–ª–∏ –∫–æ–º–∞–Ω–¥–∞ –Ω–µ –≤ –±–µ–ª–æ–º —Å–ø–∏—Å–∫–µ
    """
    if command not in ALLOWED_COMMANDS:
        raise ValueError(f"Command not allowed: {command}. Allowed: {ALLOWED_COMMANDS}")
    return True

def run_subprocess_with_logging(cmd, cwd=".", current_file="", total_files=1, file_index=0):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

    –í–ê–ñ–ù–û: –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ—Ö–æ–¥—è—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    """
    global processing_state

    # Generate unique task ID
    task_id = f"task_{current_file.replace('/', '_').replace(' ', '_')}_{int(time.time())}"

    with processing_state["lock"]:
        processing_state["is_running"] = True
        processing_state["current_file"] = current_file
        processing_state["total_files"] = total_files
        processing_state["processed_files"] = file_index
        if processing_state["start_time"] is None:
            processing_state["start_time"] = time.time()

    # Broadcast task start
    websocket_server.broadcast_message("task_created", {
        "taskId": task_id,
        "name": current_file,
        "total": total_files
    })

    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π –∫–æ–º–∞–Ω–¥—ã
        safe_cmd = []
        for part in cmd:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º shlex.quote –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            safe_cmd.append(shlex.quote(part))

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–∏
        safe_cwd = Path(cwd).resolve()
        if not safe_cwd.exists():
            raise ValueError(f"Working directory does not exist: {safe_cwd}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —Å —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        # –í–∞–∂–Ω–æ: shell=False –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è shell injection
        process = subprocess.Popen(
            cmd,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫, —Ç.–∫. shell=False
            cwd=str(safe_cwd),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            shell=False  # –ö–†–ò–¢–ò–ß–ù–û: –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º shell=True!
        )

        # –ß–∏—Ç–∞–µ–º –≤—ã–≤–æ–¥ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
        for line in iter(process.stdout.readline, ''):
            if line:
                line = line.rstrip()
                timestamp = datetime.now().strftime("%H:%M:%S")

                with processing_state["lock"]:
                    processing_state["logs"].append({
                        "timestamp": timestamp,
                        "message": line
                    })

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–µ—Ä–µ–∑ WebSocket –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                websocket_server.broadcast_message("task_log", {
                    "taskId": task_id,
                    "message": line,
                    "timestamp": timestamp
                })

        process.wait()
        returncode = process.returncode

        # Broadcast task completion
        websocket_server.broadcast_message("task_completed", {
            "taskId": task_id,
            "returncode": returncode,
            "result": {
                "success": returncode == 0
            }
        })

        return returncode

    except Exception as e:
        timestamp = datetime.now().strftime("%H:%M:%S")
        error_msg = f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"

        with processing_state["lock"]:
            processing_state["logs"].append({
                "timestamp": timestamp,
                "message": error_msg
            })

        # Broadcast error
        websocket_server.broadcast_message("task_failed", {
            "taskId": task_id,
            "error": str(e),
            "timestamp": timestamp
        })

        return 1
    finally:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        with processing_state["lock"]:
            processing_state["processed_files"] = file_index + 1

        # Broadcast progress update
        websocket_server.broadcast_message("task_progress", {
            "taskId": task_id,
            "processed": file_index + 1,
            "total": total_files,
            "progress": int((file_index + 1) / total_files * 100) if total_files > 0 else 0
        })

class EmailCheckerWebHandler(BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        self.base_dir = Path(".")
        super().__init__(*args, **kwargs)

    def do_GET(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ GET –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø—É—Ç–µ–π"""
        path = urlparse(self.path).path

        # –ë–µ–ª—ã–π —Å–ø–∏—Å–æ–∫ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö GET endpoints
        allowed_endpoints = {
            "/", "/index", "/debug", "/debug.html",
            "/new", "/v2", "/modern",  # –ù–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (SPA)
            "/old", "/classic", "/legacy",  # –°—Ç–∞—Ä—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (—è–≤–Ω—ã–π –¥–æ—Å—Ç—É–ø)
            # –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            "/index.html", "/lists.html", "/email-list.html", "/bulk-lists.html",
            "/smart-filter.html", "/blocklist.html",
            "/processing-queue.html", "/analytics.html", "/ml-analytics.html",
            "/archive.html", "/settings.html",
            "/api/lists", "/api/status", "/api/reports",
            "/api/metadata", "/api/metadata-search", "/api/lvp-sources",
            "/api/metadata-stats", "/api/country-mismatches", "/api/processing-status",
            "/api/output-files", "/api/file-preview", "/api/download-file",
            "/api/admin/stats", "/api/dashboard-stats", "/api/all-files",
            "/api/smart-filter/available", "/api/smart-filter/config",
            "/api/smart-filter/auto-suggest",
            # Template API endpoints
            "/api/templates",
            # Blocklist API endpoints
            "/api/blocklist", "/api/blocklist/stats",
            # Email Records API endpoints
            "/api/emails", "/api/emails/count"
        }

        # –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ (CSS, JS, images)
        if path.startswith("/assets/"):
            self.serve_static_file(path)
        # –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ web/
        elif path.endswith(".html") and path in allowed_endpoints:
            filename = path.lstrip("/")  # Remove leading slash
            self.serve_file(f"web/{filename}", "text/html")
        # Blocklist API endpoints with query params (process before allowed_endpoints check)
        elif path.startswith("/api/blocklist/search"):
            self.handle_blocklist_search()
        elif path.startswith("/api/blocklist/export"):
            self.handle_blocklist_export()
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ email-metadata endpoint (–æ—Å–æ–±—ã–π —Å–ª—É—á–∞–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º)
        elif path.startswith("/api/email-metadata/"):
            self.handle_get_email_metadata()
        elif path.startswith("/api/output-files"):
            self.handle_get_output_files()
        elif path.startswith("/api/file-preview"):
            self.handle_file_preview()
        elif path.startswith("/api/download-file"):
            self.handle_download_file()
        elif path in allowed_endpoints:
            # –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –°–¢–ê–†–´–ô –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if path == "/" or path == "/index":
                self.serve_file("email_list_manager.html", "text/html")
            # –Ø–≤–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å—Ç–∞—Ä–æ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É
            elif path in ["/old", "/classic", "/legacy"]:
                self.serve_file("email_list_manager.html", "text/html")
            # –ù–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (SPA)
            elif path in ["/new", "/v2", "/modern"]:
                self.serve_file("web/index.html", "text/html")
            elif path == "/debug" or path == "/debug.html":
                self.serve_file("web/debug.html", "text/html")
            elif path == "/api/lists":
                self.handle_get_lists()
            elif path == "/api/status":
                self.handle_get_status()
            elif path == "/api/reports":
                self.handle_get_reports()
            elif path == "/api/metadata":
                self.handle_get_metadata()
            elif path == "/api/metadata-search":
                self.handle_metadata_search()
            elif path == "/api/lvp-sources":
                self.handle_get_lvp_sources()
            elif path == "/api/metadata-stats":
                self.handle_get_metadata_stats()
            elif path == "/api/country-mismatches":
                self.handle_get_country_mismatches()
            elif path == "/api/processing-status":
                self.handle_get_processing_status()
            elif path == "/api/admin/stats":
                self.handle_admin_stats()
            elif path == "/api/dashboard-stats":
                self.handle_get_dashboard_stats()
            elif path == "/api/all-files":
                self.handle_get_all_files()
            elif path == "/api/smart-filter/available":
                self.handle_get_available_smart_filters()
            elif path.startswith("/api/smart-filter/config"):
                self.handle_get_smart_filter_config()
            elif path.startswith("/api/smart-filter/auto-suggest"):
                self.handle_auto_suggest_config()
            # Template API endpoints
            elif path == "/api/templates":
                self.handle_get_templates()
            elif path.startswith("/api/templates/draft/"):
                self.handle_get_draft()
            # Blocklist API endpoints (exact paths, query params handled above)
            elif path == "/api/blocklist":
                self.handle_get_blocklist()
            elif path == "/api/blocklist/stats":
                self.handle_get_blocklist_stats()
            # Email Records API endpoints
            elif path == "/api/emails":
                handle_get_emails(self)
            elif path == "/api/emails/count":
                handle_get_email_count(self)
            elif path.startswith("/api/emails/") and path != "/api/emails/count":
                # Handle single email GET: /api/emails/{email}
                email = path.split("/api/emails/", 1)[1]
                if email:
                    handle_get_single_email(self, email)
                else:
                    self.send_error(400, "Invalid email parameter")
        else:
            self.send_error(404, f"Endpoint not found: {path}")

    def do_POST(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ POST –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        path = urlparse(self.path).path

        # –ë–µ–ª—ã–π —Å–ø–∏—Å–æ–∫ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö POST endpoints
        allowed_endpoints = {
            "/api/save_list", "/api/process", "/api/process_one",
            "/api/reset_processing", "/api/metadata/add", "/api/metadata/remove",
            "/api/metadata/save", "/api/metadata/batch-update-status",
            "/api/metadata/batch-update-field", "/api/import-lvp",
            "/api/export-lvp", "/api/enrich-list", "/api/enrich-all",
            "/api/upload-file", "/api/admin/clear-cache", "/api/admin/optimize-db",
            "/api/admin/delete-file", "/api/admin/reset-system", "/api/admin/restore-data",
            "/api/smart-filter/process", "/api/smart-filter/process-batch",
            "/api/smart-filter/workflow", "/api/smart-filter/apply",
            "/api/lists/bulk-update",  # Bulk list metadata update
            "/api/scan-input-directory",  # Scan input/ directory for new files
            # Template API endpoints
            "/api/templates", "/api/templates/draft",
            # Blocklist API endpoints
            "/api/blocklist/add", "/api/blocklist/remove",
            "/api/blocklist/bulk-add", "/api/blocklist/bulk-remove",
            "/api/blocklist/import-csv",
            # Email Records API endpoints
            "/api/emails", "/api/emails/bulk-update", "/api/emails/bulk-delete",
            "/api/emails/export", "/api/emails/bulk-status"
        }

        if path in allowed_endpoints:
            if path == "/api/save_list":
                self.handle_save_list()
            elif path == "/api/process":
                self.handle_process_lists()
            elif path == "/api/process_one":
                self.handle_process_one()
            elif path == "/api/reset_processing":
                self.handle_reset_processing()
            elif path == "/api/lists/bulk-update":
                self.handle_lists_bulk_update()
            elif path == "/api/scan-input-directory":
                self.handle_scan_input_directory()
            elif path == "/api/upload-file":
                self.handle_upload_file()
            elif path == "/api/admin/clear-cache":
                self.handle_admin_clear_cache()
            elif path == "/api/admin/optimize-db":
                self.handle_admin_optimize_db()
            elif path == "/api/admin/delete-file":
                self.handle_admin_delete_file()
            elif path == "/api/admin/reset-system":
                self.handle_admin_reset_system()
            elif path == "/api/admin/restore-data":
                self.handle_admin_restore_data()
            elif path == "/api/metadata/add":
                self.handle_add_metadata()
            elif path == "/api/metadata/remove":
                self.handle_remove_metadata()
            elif path == "/api/metadata/save":
                self.handle_save_metadata()
            elif path == "/api/metadata/batch-update-status":
                self.handle_batch_update_status()
            elif path == "/api/metadata/batch-update-field":
                self.handle_batch_update_field()
            elif path == "/api/import-lvp":
                self.handle_import_lvp()
            elif path == "/api/export-lvp":
                self.handle_export_lvp()
            elif path == "/api/enrich-list":
                self.handle_enrich_list()
            elif path == "/api/enrich-all":
                self.handle_enrich_all()
            elif path == "/api/smart-filter/process":
                self.handle_smart_filter_process()
            elif path == "/api/smart-filter/process-batch":
                self.handle_smart_filter_process_batch()
            elif path == "/api/smart-filter/workflow":
                self.handle_smart_filter_workflow()
            elif path == "/api/smart-filter/apply":
                self.handle_smart_filter_apply()
            # Template API POST handlers
            elif path == "/api/templates":
                self.handle_save_template()
            elif path == "/api/templates/draft":
                self.handle_save_draft()
            # Blocklist API POST handlers
            elif path == "/api/blocklist/add":
                self.handle_post_blocklist_add()
            elif path == "/api/blocklist/remove":
                self.handle_post_blocklist_remove()
            elif path == "/api/blocklist/bulk-add":
                self.handle_post_blocklist_bulk_add()
            elif path == "/api/blocklist/bulk-remove":
                self.handle_post_blocklist_bulk_remove()
            elif path == "/api/blocklist/import-csv":
                self.handle_post_blocklist_import_csv()
            # Email Records API endpoints
            elif path == "/api/emails":
                handle_get_emails(self)  # POST to /api/emails can also get paginated results
            elif path == "/api/emails/bulk-update":
                handle_bulk_update_emails(self)
            elif path == "/api/emails/bulk-delete":
                handle_bulk_delete_emails(self)
            elif path == "/api/emails/export":
                handle_export_emails(self)
            elif path == "/api/emails/bulk-status":
                handle_bulk_status_update(self)
            else:
                self.send_error(501, f"Handler not implemented for: {path}")
        else:
            self.send_error(404, f"Endpoint not found: {path}")

    def do_DELETE(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ DELETE –∑–∞–ø—Ä–æ—Å–æ–≤"""
        path = urlparse(self.path).path

        # Handle DELETE /api/templates/{template_id}
        if path.startswith("/api/templates/") and not path.startswith("/api/templates/draft/"):
            self.handle_delete_template()
        # Handle DELETE /api/emails/{email}
        elif path.startswith("/api/emails/"):
            email = path.split("/api/emails/", 1)[1]
            if email:
                handle_delete_email(self, email)
            else:
                self.send_error(400, "Invalid email parameter")
        else:
            self.send_error(404, f"DELETE endpoint not found: {path}")

    def serve_file(self, filename, content_type):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∞–π–ª–∞ –∫–ª–∏–µ–Ω—Ç—É"""
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ - —Ä–∞–∑—Ä–µ—à–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–æ –Ω–µ path traversal
            if ".." in filename or filename.startswith("/"):
                raise ValueError("Path traversal attempt detected")

            file_path = self.base_dir / filename

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Ñ–∞–π–ª –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            file_path = file_path.resolve()
            base_path = self.base_dir.resolve()

            if not str(file_path).startswith(str(base_path)):
                raise ValueError("Path traversal attempt detected")

            if file_path.exists() and file_path.is_file():
                with open(file_path, 'rb') as f:
                    content = f.read()

                self.send_response(200)
                self.send_header("Content-Type", content_type)
                self.send_header("Content-Length", len(content))
                # –û—Ç–∫–ª—é—á–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ HTML —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
                self.send_header("Cache-Control", "no-cache, no-store, must-revalidate")
                self.send_header("Pragma", "no-cache")
                self.send_header("Expires", "0")
                self.end_headers()
                self.wfile.write(content)
            else:
                self.send_error(404, f"File not found: {filename}")
        except Exception as e:
            print(f"Error serving file: {e}")

    def serve_static_file(self, path):
        """–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ (CSS, JS, images) –∏–∑ –ø–∞–ø–∫–∏ assets"""
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ - —Ä–∞–∑—Ä–µ—à–∞–µ–º —Ç–æ–ª—å–∫–æ /assets/*
            if ".." in path or path.count("//") > 1:
                raise ValueError("Path traversal attempt detected")

            # –£–¥–∞–ª—è–µ–º /assets/ –∏–∑ –Ω–∞—á–∞–ª–∞ –ø—É—Ç–∏
            asset_path = path[8:]  # –£–±–∏—Ä–∞–µ–º "/assets/"
            file_path = self.base_dir / "web" / "assets" / asset_path

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å - –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤ assets
            file_path = file_path.resolve()
            assets_path = (self.base_dir / "web" / "assets").resolve()

            if not str(file_path).startswith(str(assets_path)):
                raise ValueError("Path traversal attempt detected")

            if file_path.exists() and file_path.is_file():
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º content-type –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
                content_type_map = {
                    '.css': 'text/css',
                    '.js': 'application/javascript',
                    '.png': 'image/png',
                    '.jpg': 'image/jpeg',
                    '.jpeg': 'image/jpeg',
                    '.gif': 'image/gif',
                    '.svg': 'image/svg+xml',
                    '.woff': 'font/woff',
                    '.woff2': 'font/woff2',
                    '.ttf': 'font/ttf',
                    '.json': 'application/json',
                    '.html': 'text/html',
                }
                file_ext = file_path.suffix.lower()
                content_type = content_type_map.get(file_ext, 'application/octet-stream')

                with open(file_path, 'rb') as f:
                    content = f.read()

                self.send_response(200)
                self.send_header("Content-Type", content_type)
                self.send_header("Content-Length", len(content))
                # –ö–æ—Ä–æ—Ç–∫–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (30 —Å–µ–∫—É–Ω–¥)
                self.send_header("Cache-Control", "max-age=30")
                self.end_headers()
                self.wfile.write(content)
            else:
                self.send_error(404, f"Asset not found: {path}")
        except Exception as e:
            print(f"Error serving static file: {e}")
            self.send_error(500, f"Error: {str(e)}")
            self.send_error(500, str(e))

    def send_json_response(self, data, status_code=200):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ JSON –æ—Ç–≤–µ—Ç–∞"""
        try:
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            self.send_response(status_code)
            self.send_header("Content-Type", "application/json; charset=utf-8")
            self.send_header("Content-Length", len(json_data.encode()))
            self.send_header("Access-Control-Allow-Origin", "*")
            self.end_headers()
            self.wfile.write(json_data.encode())
        except Exception as e:
            print(f"Error sending JSON response: {e}")

    def count_lines_in_file(self, file_path):
        """–ü–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ"""
        try:
            if not file_path.exists():
                return 0
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return sum(1 for line in f if line.strip())
        except Exception as e:
            print(f"Error counting lines in {file_path}: {e}")
            return 0

    def count_lines_in_output(self, filename_base):
        """–ü–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–∞—Ö output –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
        try:
            output_dir = self.base_dir / "output"
            if not output_dir.exists():
                return 0, 0

            clean_count = 0
            blocked_count = 0

            # –ò—â–µ–º —Ñ–∞–π–ª—ã clean –∏ blocked –¥–ª—è —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞
            for output_file in output_dir.glob(f"{filename_base}_clean_*.txt"):
                clean_count += self.count_lines_in_file(output_file)

            for output_file in output_dir.glob(f"{filename_base}_blocked_*.txt"):
                blocked_count += self.count_lines_in_file(output_file)

            return clean_count, blocked_count
        except Exception as e:
            print(f"Error counting output lines for {filename_base}: {e}")
            return 0, 0

    def handle_get_lists(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ email —Ñ–∞–π–ª–æ–≤"""
        try:
            config_file = self.base_dir / "lists_config.json"
            lists_data = []

            if config_file.exists():
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
                if config_file.stat().st_size > 10 * 1024 * 1024:  # 10MB max
                    raise ValueError("Config file too large")

                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    lists_data = config.get("lists", [])

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ø–∏—Å–∫–∞
            input_dir = self.base_dir / "input"
            for list_item in lists_data:
                filename = list_item.get("filename", "")
                if not filename:
                    continue

                # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ email –≤ input —Ñ–∞–π–ª–µ
                input_file = input_dir / filename
                list_item["emails"] = self.count_lines_in_file(input_file)

                # –ü–æ–¥—Å—á–µ—Ç clean –∏ blocked –∏–∑ output —Ñ–∞–π–ª–æ–≤
                filename_base = filename.rsplit('.', 1)[0]  # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                clean_count, blocked_count = self.count_lines_in_output(filename_base)
                list_item["clean"] = clean_count
                list_item["blocked"] = blocked_count

            self.send_json_response({"lists": lists_data})
        except json.JSONDecodeError as e:
            print(f"Invalid JSON in config file: {e}")
            self.send_json_response({"error": "Invalid configuration file"}, 500)
        except Exception as e:
            print(f"Error getting lists: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_status(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –∫–æ–º–∞–Ω–¥—É –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            result = subprocess.run(
                ["python3", "email_checker.py", "status"],
                capture_output=True,
                text=True,
                cwd=str(self.base_dir),
                timeout=30,  # –¢–∞–π–º–∞—É—Ç –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≤–∏—Å–∞–Ω–∏—è
                shell=False  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            )

            self.send_json_response({
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            })
        except subprocess.TimeoutExpired:
            self.send_json_response({"error": "Command timeout"}, 408)
        except Exception as e:
            print(f"Error getting status: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_scan_input_directory(self):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ input/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º EmailChecker
            checker = EmailChecker(str(self.base_dir))

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_file = self.base_dir / "lists_config.json"
            existing_files = set()

            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    existing_files = {item['filename'] for item in config.get('lists', [])}

            # –°–∫–∞–Ω–∏—Ä—É–µ–º input/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            input_dir = self.base_dir / "input"
            new_files = []

            if input_dir.exists():
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ .txt –∏ .lvp —Ñ–∞–π–ª—ã
                all_files = list(input_dir.glob('*.txt')) + list(input_dir.glob('*.lvp'))

                for filepath in all_files:
                    filename = filepath.name

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    if filename in existing_files:
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –∞–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
                    try:
                        metadata = checker.config_manager.get_list_metadata(filename, checker.output_dir)

                        # –î–æ–±–∞–≤–ª—è–µ–º file_type –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                        if 'file_type' not in metadata:
                            metadata['file_type'] = 'lvp' if filename.endswith('.lvp') else 'txt'

                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                        metadata['file_size'] = filepath.stat().st_size

                        new_files.append(metadata)
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –Ω–æ–≤—ã–π —Ñ–∞–π–ª: {filename} (Country: {metadata.get('country')}, Category: {metadata.get('category')})")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
                        continue

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.send_json_response({
                "success": True,
                "new_files_count": len(new_files),
                "new_files": new_files,
                "message": f"–ù–∞–π–¥–µ–Ω–æ {len(new_files)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤" if new_files else "–ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            })

            print(f"üìä –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: –Ω–∞–π–¥–µ–Ω–æ {len(new_files)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")

        except json.JSONDecodeError as e:
            print(f"Invalid JSON in config file: {e}")
            self.send_json_response({"error": "Invalid configuration file"}, 500)
        except Exception as e:
            print(f"Error scanning input directory: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    def handle_process_one(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
        try:
            content_length = int(self.headers['Content-Length'])

            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ (1MB max)
            if content_length > 1024 * 1024:
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            filename = data.get("filename", "").strip()
            if not filename:
                self.send_json_response({"error": "–ù–µ —É–∫–∞–∑–∞–Ω filename"}, 400)
                return

            # –ö–†–ò–¢–ò–ß–ù–û: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            try:
                validate_filename(filename)
            except ValueError as e:
                self.send_json_response({"error": f"Invalid filename: {str(e)}"}, 400)
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ lists_config.json
            force_reprocess = data.get("force_reprocess", False)
            config_file = self.base_dir / "lists_config.json"

            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        lists = config.get("lists", [])

                        # –ù–∞—Ö–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                        list_entry = next((l for l in lists if l.get("filename") == filename), None)

                        if list_entry and list_entry.get("processed") == True:
                            # –°–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                            if not force_reprocess:
                                self.send_json_response({
                                    "error": "–°–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ force_reprocess=true –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.",
                                    "already_processed": True
                                }, 400)
                                return
                            else:
                                print(f"‚ö†Ô∏è –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞: {filename}")
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è lists_config.json: {e}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            input_file = self.base_dir / "input" / filename
            if not input_file.exists():
                self.send_json_response({"error": f"File not found: {filename}"}, 404)
                return

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –≤—ã–±–∏—Ä–∞–µ–º –∫–æ–º–∞–Ω–¥—É
            file_extension = Path(filename).suffix.lower()

            if file_extension == ".lvp":
                command = "check-lvp"
            else:
                command = "check"

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥—ã
            try:
                validate_command(command)
            except ValueError as e:
                self.send_json_response({"error": str(e)}, 400)
                return

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            def run_processing():
                global processing_state

                # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
                with processing_state["lock"]:
                    processing_state["logs"].clear()
                    processing_state["start_time"] = None
                    processing_state["processed_files"] = 0

                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
                # –í–∞–∂–Ω–æ: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º f-—Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é!
                safe_input_path = str(Path("input") / filename)

                returncode = run_subprocess_with_logging(
                    [sys.executable, "email_checker.py", command, safe_input_path],
                    cwd=str(self.base_dir),
                    current_file=filename,
                    total_files=1,
                    file_index=0
                )

                # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                with processing_state["lock"]:
                    processing_state["is_running"] = False
                    if returncode == 0:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ {filename} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
                        })
                    else:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ {filename} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –∫–æ–¥–æ–º: {returncode}"
                        })

            thread = threading.Thread(target=run_processing)
            thread.daemon = True  # –ü–æ—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
            thread.start()

            self.send_json_response({
                "success": True,
                "message": f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {filename} –∑–∞–ø—É—â–µ–Ω–∞ (—Ç–∏–ø: {file_extension})"
            })
        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error processing one file: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_processing_status(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        global processing_state

        with processing_state["lock"]:
            elapsed_time = 0
            if processing_state["start_time"]:
                elapsed_time = time.time() - processing_state["start_time"]

            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –ª–æ–≥–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            recent_logs = processing_state["logs"][-100:] if len(processing_state["logs"]) > 100 else processing_state["logs"]

            status = {
                "is_running": processing_state["is_running"],
                "logs": recent_logs,
                "current_file": processing_state.get("current_file", ""),
                "processed_files": processing_state.get("processed_files", 0),
                "total_files": processing_state.get("total_files", 0),
                "elapsed_time": int(elapsed_time)
            }

        self.send_json_response(status)

    def handle_process_lists(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–ø–∏—Å–∫–æ–≤"""
        global processing_state

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–¥–µ—Ç –ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞
            with processing_state["lock"]:
                if processing_state["is_running"]:
                    self.send_json_response({
                        "success": False,
                        "error": "–û–±—Ä–∞–±–æ—Ç–∫–∞ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–∞. –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è."
                    }, 409)
                    return

            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            mode = "check-all-incremental"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            exclude_duplicates = True
            generate_html = True

            try:
                content_length = int(self.headers.get('Content-Length', 0))
                if content_length > 0:
                    if content_length > 1024 * 1024:  # 1MB max
                        self.send_json_response({"error": "Request too large"}, 413)
                        return

                    post_data = self.rfile.read(content_length)
                    data = json.loads(post_data.decode())

                    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                    mode = data.get("mode", "check-all-incremental")
                    exclude_duplicates = data.get("exclude_duplicates", True)
                    generate_html = data.get("generate_html", True)
            except json.JSONDecodeError:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ JSON - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                pass

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞
            try:
                validate_command(mode)
            except ValueError as e:
                self.send_json_response({"error": f"Invalid mode: {str(e)}"}, 400)
                return

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É
            cmd = [sys.executable, "email_checker.py", mode]

            if exclude_duplicates:
                cmd.append("--exclude-duplicates")

            if generate_html:
                cmd.append("--generate-html")

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            def run_processing():
                global processing_state

                # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
                with processing_state["lock"]:
                    processing_state["logs"].clear()
                    processing_state["start_time"] = None
                    processing_state["processed_files"] = 0
                    processing_state["total_files"] = 0

                returncode = run_subprocess_with_logging(
                    cmd,
                    cwd=str(self.base_dir),
                    current_file="–í—Å–µ —Ñ–∞–π–ª—ã",
                    total_files=1,
                    file_index=0
                )

                # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                with processing_state["lock"]:
                    processing_state["is_running"] = False
                    if returncode == 0:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": "‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
                        })
                    else:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –∫–æ–¥–æ–º: {returncode}"
                        })

            thread = threading.Thread(target=run_processing)
            thread.daemon = True
            thread.start()

            message = f"–ó–∞–ø—É—â–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∂–∏–º–µ: {mode}"
            if exclude_duplicates:
                message += " (—Å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)"
            if generate_html:
                message += " (—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π HTML –æ—Ç—á–µ—Ç–∞)"

            self.send_json_response({
                "success": True,
                "message": message
            })

        except Exception as e:
            print(f"Error starting processing: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_save_list(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–∞"""
        try:
            content_length = int(self.headers['Content-Length'])

            if content_length > 1024 * 1024:  # 1MB max
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            filename = data.get("filename", "").strip()
            if not filename:
                self.send_json_response({"error": "Filename is required"}, 400)
                return

            try:
                validate_filename(filename)
            except ValueError as e:
                self.send_json_response({"error": f"Invalid filename: {str(e)}"}, 400)
                return

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_file = self.base_dir / "lists_config.json"
            lists = []

            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    lists = config.get("lists", [])

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫
            found = False
            for i, lst in enumerate(lists):
                if lst.get("filename") == filename:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
                    lists[i].update(data)
                    found = True
                    break

            if not found:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
                lists.append(data)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump({"lists": lists}, f, ensure_ascii=False, indent=2)

            self.send_json_response({"success": True})

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error saving list config: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_reset_processing(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–±—Ä–æ—Å —Ñ–ª–∞–≥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            config_file = self.base_dir / "lists_config.json"

            if not config_file.exists():
                self.send_json_response({"error": "Config file not found"}, 404)
                return

            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥–∏ processed
            for lst in config.get("lists", []):
                lst["processed"] = False

            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)

            self.send_json_response({"success": True, "message": "–§–ª–∞–≥–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–±—Ä–æ—à–µ–Ω—ã"})

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid config file"}, 500)
        except Exception as e:
            print(f"Error resetting processing flags: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_lists_bulk_update(self):
        """–ú–∞—Å—Å–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤"""
        try:
            content_length = int(self.headers['Content-Length'])

            if content_length > 1024 * 1024:  # 1MB max
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            filenames = data.get("filenames", [])
            updates = data.get("updates", {})

            if not isinstance(filenames, list):
                self.send_json_response({"error": "filenames must be an array"}, 400)
                return

            if not filenames:
                self.send_json_response({"error": "filenames array is empty"}, 400)
                return

            if not isinstance(updates, dict):
                self.send_json_response({"error": "updates must be an object"}, 400)
                return

            if not updates:
                self.send_json_response({"error": "updates object is empty"}, 400)
                return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
            for filename in filenames:
                if not isinstance(filename, str):
                    self.send_json_response({"error": f"Invalid filename type: {type(filename)}"}, 400)
                    return
                try:
                    validate_filename(filename)
                except ValueError as e:
                    self.send_json_response({"error": f"Invalid filename '{filename}': {str(e)}"}, 400)
                    return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
            allowed_fields = {"country", "category", "priority", "processed", "description", "display_name"}
            for field in updates.keys():
                if field not in allowed_fields:
                    self.send_json_response({"error": f"Field '{field}' is not allowed for update"}, 400)
                    return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π
            if "priority" in updates:
                priority = updates["priority"]
                if not isinstance(priority, int) or priority < 50 or priority > 999:
                    self.send_json_response({"error": "priority must be integer between 50 and 999"}, 400)
                    return

            if "country" in updates:
                country = updates["country"]
                if not isinstance(country, str) or not country.strip():
                    self.send_json_response({"error": "country must be non-empty string"}, 400)
                    return

            if "category" in updates:
                category = updates["category"]
                if not isinstance(category, str) or not category.strip():
                    self.send_json_response({"error": "category must be non-empty string"}, 400)
                    return

            if "processed" in updates:
                processed = updates["processed"]
                if not isinstance(processed, bool):
                    self.send_json_response({"error": "processed must be boolean"}, 400)
                    return

            if "description" in updates:
                description = updates["description"]
                if not isinstance(description, str):
                    self.send_json_response({"error": "description must be string"}, 400)
                    return

            if "display_name" in updates:
                display_name = updates["display_name"]
                if not isinstance(display_name, str):
                    self.send_json_response({"error": "display_name must be string"}, 400)
                    return

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_file = self.base_dir / "lists_config.json"

            if not config_file.exists():
                self.send_json_response({"error": "Config file not found"}, 404)
                return

            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)

            lists = config.get("lists", [])

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            results = []
            updated_count = 0
            failed_count = 0
            errors = []

            for filename in filenames:
                found = False
                for lst in lists:
                    if lst.get("filename") == filename:
                        found = True
                        try:
                            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                            for field, value in updates.items():
                                lst[field] = value

                            results.append({
                                "filename": filename,
                                "success": True
                            })
                            updated_count += 1
                            print(f"‚úÖ Updated list: {filename}")
                        except Exception as e:
                            error_msg = f"Failed to update {filename}: {str(e)}"
                            results.append({
                                "filename": filename,
                                "success": False,
                                "error": str(e)
                            })
                            errors.append(error_msg)
                            failed_count += 1
                            print(f"‚ùå {error_msg}")
                        break

                if not found:
                    error_msg = f"List not found: {filename}"
                    results.append({
                        "filename": filename,
                        "success": False,
                        "error": "List not found"
                    })
                    errors.append(error_msg)
                    failed_count += 1
                    print(f"‚ö†Ô∏è {error_msg}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if updated_count > 0:
                try:
                    with open(config_file, 'w', encoding='utf-8') as f:
                        json.dump(config, f, ensure_ascii=False, indent=2)
                    print(f"üíæ Saved config with {updated_count} updates")
                except Exception as e:
                    self.send_json_response({
                        "success": False,
                        "error": f"Failed to save config: {str(e)}"
                    }, 500)
                    return

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = {
                "success": failed_count == 0,
                "updated": updated_count,
                "failed": failed_count,
                "errors": errors,
                "results": results
            }

            self.send_json_response(response)

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON in request"}, 400)
        except Exception as e:
            print(f"Error in bulk update: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_reports(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö HTML –æ—Ç—á–µ—Ç–æ–≤"""
        try:
            output_dir = self.base_dir / "output"
            if not output_dir.exists():
                self.send_json_response({"reports": []})
                return

            # –ò—â–µ–º HTML –æ—Ç—á–µ—Ç—ã
            reports = []
            for report_file in output_dir.glob("*_report_*.html"):
                reports.append({
                    "filename": report_file.name,
                    "size": report_file.stat().st_size,
                    "modified": report_file.stat().st_mtime
                })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
            reports.sort(key=lambda x: x["modified"], reverse=True)

            self.send_json_response({"reports": reports})

        except Exception as e:
            print(f"Error getting reports: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_output_files(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∏–ª–∏ –≤—Å–µ—Ö clean —Ñ–∞–π–ª–æ–≤"""
        try:
            # –ü–∞—Ä—Å–∏–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            parsed = urlparse(self.path)
            query_params = parse_qs(parsed.query)

            list_name = query_params.get('list', [''])[0].strip()

            output_dir = self.base_dir / "output"
            if not output_dir.exists():
                self.send_json_response({"files": []})
                return

            # –ï—Å–ª–∏ list_name –Ω–µ —É–∫–∞–∑–∞–Ω - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ clean —Ñ–∞–π–ª—ã (–¥–ª—è Filter Wizard)
            if not list_name:
                clean_files = []
                for file_path in output_dir.glob("*_clean_*.txt"):
                    file_info = {
                        "filename": file_path.name,
                        "size": file_path.stat().st_size,
                        "modified": file_path.stat().st_mtime,
                        "path": str(file_path.relative_to(self.base_dir))
                    }
                    # –ü–æ–¥—Å—á–µ—Ç email –≤ —Ñ–∞–π–ª–µ
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            line_count = sum(1 for line in f if line.strip())
                        file_info["email_count"] = line_count
                    except Exception:
                        file_info["email_count"] = 0

                    clean_files.append(file_info)

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
                clean_files.sort(key=lambda x: x["modified"], reverse=True)

                self.send_json_response({"files": clean_files})
                return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            try:
                validate_filename(list_name)
            except ValueError as e:
                self.send_json_response({"error": f"Invalid list name: {str(e)}"}, 400)
                return

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            base_name = Path(list_name).stem

            # –ò—â–µ–º –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
            output_files = {
                "clean": [],
                "blocked_email": [],
                "blocked_domain": [],
                "invalid": [],
                "metadata_json": [],
                "metadata_csv": [],
                "enriched_json": [],
                "enriched_csv": [],
                "report_html": []
            }

            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–∏—Å–∫–∞
            patterns = {
                "clean": f"{base_name}_clean*.txt",
                "blocked_email": f"{base_name}_blocked_email*.txt",
                "blocked_domain": f"{base_name}_blocked_domain*.txt",
                "invalid": f"{base_name}_invalid*.txt",
                "metadata_json": f"{base_name}*metadata*.json",
                "metadata_csv": f"{base_name}*metadata*.csv",
                "enriched_json": f"{base_name}*enriched*.json",
                "enriched_csv": f"{base_name}*enriched*.csv",
                "report_html": f"{base_name}*report*.html"
            }

            for category, pattern in patterns.items():
                for file_path in output_dir.glob(pattern):
                    file_info = {
                        "filename": file_path.name,
                        "size": file_path.stat().st_size,
                        "modified": file_path.stat().st_mtime,
                        "path": str(file_path.relative_to(self.base_dir))
                    }
                    output_files[category].append(file_info)

            # –ü–æ–¥—Å—á–µ—Ç email –≤ —Ñ–∞–π–ª–∞—Ö
            for category in ["clean", "blocked_email", "blocked_domain", "invalid"]:
                for file_info in output_files[category]:
                    try:
                        file_path = self.base_dir / file_info["path"]
                        with open(file_path, 'r', encoding='utf-8') as f:
                            line_count = sum(1 for line in f if line.strip())
                        file_info["email_count"] = line_count
                    except Exception:
                        file_info["email_count"] = 0

            self.send_json_response({"files": output_files, "list_name": list_name})

        except Exception as e:
            print(f"Error getting output files: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_file_preview(self):
        """–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ (–ø–µ—Ä–≤—ã–µ N —Å—Ç—Ä–æ–∫)"""
        try:
            # –ü–∞—Ä—Å–∏–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            parsed = urlparse(self.path)
            query_params = parse_qs(parsed.query)

            file_path = query_params.get('path', [''])[0].strip()
            max_lines = int(query_params.get('lines', ['100'])[0])

            if not file_path:
                self.send_json_response({"error": "File path is required"}, 400)
                return

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            if max_lines > 1000:
                max_lines = 1000

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
            safe_path = Path(file_path)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∏ –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã base_dir
            if safe_path.is_absolute():
                self.send_json_response({"error": "Absolute paths not allowed"}, 400)
                return

            full_path = (self.base_dir / safe_path).resolve()
            base_path = self.base_dir.resolve()

            if not str(full_path).startswith(str(base_path)):
                self.send_json_response({"error": "Path traversal attempt detected"}, 400)
                return

            if not full_path.exists():
                self.send_json_response({"error": "File not found"}, 404)
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ñ–∞–π–ª –≤ output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            output_dir = self.base_dir / "output"
            if not str(full_path).startswith(str(output_dir.resolve())):
                self.send_json_response({"error": "Access denied: file not in output directory"}, 403)
                return

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
            file_extension = full_path.suffix.lower()

            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            content_lines = []
            total_lines = 0
            file_size = full_path.stat().st_size

            if file_extension == '.json':
                # –î–ª—è JSON —á–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É)
                if file_size > 5 * 1024 * 1024:  # 5MB max
                    self.send_json_response({"error": "File too large for preview"}, 413)
                    return

                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å JSON
                        json_data = json.loads(content)
                        content = json.dumps(json_data, ensure_ascii=False, indent=2)
                    except json.JSONDecodeError:
                        pass  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å

                content_lines = content.split('\n')[:max_lines]
                total_lines = len(content.split('\n'))

            elif file_extension == '.csv':
                # –î–ª—è CSV —á–∏—Ç–∞–µ–º –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
                with open(full_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i < max_lines:
                            content_lines.append(line.rstrip())
                        total_lines = i + 1

            else:
                # –î–ª—è TXT –∏ –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤
                with open(full_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i < max_lines:
                            content_lines.append(line.rstrip())
                        total_lines = i + 1

            self.send_json_response({
                "content": '\n'.join(content_lines),
                "total_lines": total_lines,
                "preview_lines": len(content_lines),
                "truncated": total_lines > max_lines,
                "file_size": file_size,
                "file_type": file_extension
            })

        except UnicodeDecodeError:
            self.send_json_response({"error": "File encoding error"}, 400)
        except Exception as e:
            print(f"Error previewing file: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_download_file(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        try:
            # –ü–∞—Ä—Å–∏–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            parsed = urlparse(self.path)
            query_params = parse_qs(parsed.query)

            file_path = query_params.get('path', [''])[0].strip()

            if not file_path:
                self.send_json_response({"error": "File path is required"}, 400)
                return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
            safe_path = Path(file_path)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
            if safe_path.is_absolute():
                self.send_json_response({"error": "Absolute paths not allowed"}, 400)
                return

            full_path = (self.base_dir / safe_path).resolve()
            base_path = self.base_dir.resolve()

            if not str(full_path).startswith(str(base_path)):
                self.send_json_response({"error": "Path traversal attempt detected"}, 400)
                return

            if not full_path.exists():
                self.send_json_response({"error": "File not found"}, 404)
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ñ–∞–π–ª –≤ output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            output_dir = self.base_dir / "output"
            if not str(full_path).startswith(str(output_dir.resolve())):
                self.send_json_response({"error": "Access denied: file not in output directory"}, 403)
                return

            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(full_path, 'rb') as f:
                content = f.read()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º MIME —Ç–∏–ø
            mime_types = {
                '.txt': 'text/plain',
                '.csv': 'text/csv',
                '.json': 'application/json',
                '.html': 'text/html',
                '.lvp': 'application/xml'
            }
            mime_type = mime_types.get(full_path.suffix.lower(), 'application/octet-stream')

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª
            self.send_response(200)
            self.send_header("Content-Type", mime_type)
            self.send_header("Content-Length", len(content))
            self.send_header("Content-Disposition", f'attachment; filename="{full_path.name}"')
            self.end_headers()
            self.wfile.write(content)

        except Exception as e:
            print(f"Error downloading file: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_upload_file(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é input/"""
        try:
            import cgi

            # SECURITY: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –ü–ï–†–ï–î —á—Ç–µ–Ω–∏–µ–º –≤ –ø–∞–º—è—Ç—å
            max_size = 100 * 1024 * 1024  # 100MB
            content_length = int(self.headers.get('Content-Length', 0))

            if content_length > max_size:
                self.send_json_response({
                    "error": f"File too large: {content_length / (1024*1024):.1f}MB. Max: {max_size / (1024*1024):.0f}MB"
                }, 413)
                return

            if content_length == 0:
                self.send_json_response({"error": "Content-Length is 0 or missing"}, 400)
                return

            # –ü–æ–ª—É—á–∞–µ–º Content-Type header
            content_type = self.headers.get('Content-Type', '')

            if 'multipart/form-data' not in content_type:
                self.send_json_response({"error": "Invalid content type, expected multipart/form-data"}, 400)
                return

            # –ü–∞—Ä—Å–∏–º multipart/form-data
            form = cgi.FieldStorage(
                fp=self.rfile,
                headers=self.headers,
                environ={
                    'REQUEST_METHOD': 'POST',
                    'CONTENT_TYPE': content_type,
                }
            )

            if 'file' not in form:
                self.send_json_response({"error": "No file uploaded"}, 400)
                return

            fileitem = form['file']

            if not fileitem.filename:
                self.send_json_response({"error": "No filename provided"}, 400)
                return

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            filename = Path(fileitem.filename).name  # –¢–æ–ª—å–∫–æ –∏–º—è, –±–µ–∑ –ø—É—Ç–∏

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            try:
                validate_filename(filename)
            except ValueError as e:
                self.send_json_response({"error": f"Invalid filename: {str(e)}"}, 400)
                return

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            allowed_extensions = {'.txt', '.lvp'}
            file_ext = Path(filename).suffix.lower()
            if file_ext not in allowed_extensions:
                self.send_json_response({
                    "error": f"Invalid file type: {file_ext}. Allowed: .txt, .lvp"
                }, 400)
                return

            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            file_data = fileitem.file.read()

            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (100MB max)
            max_size = 100 * 1024 * 1024  # 100MB
            if len(file_data) > max_size:
                self.send_json_response({
                    "error": f"File too large: {len(file_data)} bytes. Max: {max_size} bytes"
                }, 413)
                return

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            if len(file_data) == 0:
                self.send_json_response({"error": "File is empty"}, 400)
                return

            # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            input_dir = self.base_dir / "input"
            input_dir.mkdir(exist_ok=True)

            target_path = input_dir / filename

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ overwrite
            overwrite = form.getvalue('overwrite', 'false').lower() == 'true'

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å
            if target_path.exists() and not overwrite:
                self.send_json_response({
                    "error": f"File {filename} already exists. Please rename or delete the existing file first."
                }, 409)
                return

            # –ï—Å–ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∞, –ª–æ–≥–∏—Ä—É–µ–º
            if target_path.exists() and overwrite:
                print(f"‚ö†Ô∏è Overwriting existing file: {filename}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            with open(target_path, 'wb') as f:
                f.write(file_data)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lists_config.json
            config_file = self.base_dir / "lists_config.json"
            lists = []

            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    lists = config.get("lists", [])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—å –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            file_exists_in_config = any(lst.get("filename") == filename for lst in lists)

            if not file_exists_in_config:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º ConfigManager –¥–ª—è –∞–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                try:
                    from email_checker_core.config import ConfigManager
                    
                    config_manager = ConfigManager(str(self.base_dir))
                    output_dir = self.base_dir / "output"
                    metadata = config_manager.get_list_metadata(filename, output_dir)

                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    metadata["file_size"] = len(file_data)
                    metadata["description"] = f"Uploaded on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    metadata["file_type"] = file_ext[1:]  # –±–µ–∑ —Ç–æ—á–∫–∏

                    new_list = metadata
                    print(f"‚úÖ File metadata auto-detected: country={metadata.get('country')}, category={metadata.get('category')}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Error auto-detecting metadata, using defaults: {e}")
                    # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –≤–∞—Ä–∏–∞–Ω—Ç—É
                    new_list = {
                        "filename": filename,
                        "display_name": Path(filename).stem,
                        "file_type": file_ext[1:],  # –±–µ–∑ —Ç–æ—á–∫–∏
                        "country": "Unknown",
                        "category": "General",
                        "priority": len(lists) + 1,
                        "processed": False,
                        "date_added": datetime.now().strftime("%Y-%m-%d"),
                        "file_size": len(file_data),
                        "description": f"Uploaded on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    }

                lists.append(new_list)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump({"lists": lists}, f, ensure_ascii=False, indent=2)

            self.send_json_response({
                "success": True,
                "message": f"File {filename} uploaded successfully",
                "filename": filename,
                "size": len(file_data),
                "type": file_ext
            })

        except Exception as e:
            print(f"Error uploading file: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    # ===== METADATA HANDLERS =====

    def handle_get_metadata(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            metadata_file = self.base_dir / "metadata_config.json"
            countries = set()
            categories = set()

            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    countries.update(metadata.get("countries", []))
                    categories.update(metadata.get("categories", []))
            else:
                # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –±–∞–∑–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                default_metadata = {
                    "countries": ["Unknown", "Mixed", "Europe"],
                    "categories": ["General"]
                }
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(default_metadata, f, indent=2, ensure_ascii=False)
                countries.update(default_metadata["countries"])
                categories.update(default_metadata["categories"])

            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–ø–∏—Å–∫–æ–≤
            config_file = self.base_dir / "lists_config.json"
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)

                for list_info in config.get("lists", []):
                    countries.add(list_info.get("country", "Unknown"))
                    categories.add(list_info.get("category", "General"))

            self.send_json_response({
                "countries": sorted(list(countries)),
                "categories": sorted(list(categories))
            })
        except Exception as e:
            print(f"Error getting metadata: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_email_metadata(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö email –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
            path = urlparse(self.path).path
            filename = path.split("/api/email-metadata/")[1]

            # –ò—â–µ–º —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ output
            output_dir = self.base_dir / "output"
            metadata_file = output_dir / filename

            if not metadata_file.exists():
                self.send_json_response({"error": "Metadata file not found"}, 404)
                return

            # –ß–∏—Ç–∞–µ–º JSON —Ñ–∞–π–ª
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            self.send_json_response(metadata)

        except Exception as e:
            print(f"Error getting email metadata: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_add_metadata(self):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω—ã –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            metadata_type = data.get("type")  # "country" –∏–ª–∏ "category"
            value = data.get("value", "").strip()

            if not metadata_type or not value:
                self.send_json_response({"error": "–ù–µ —É–∫–∞–∑–∞–Ω type –∏–ª–∏ value"}, 400)
                return

            if metadata_type not in ["country", "category"]:
                self.send_json_response({"error": "–ù–µ–≤–µ—Ä–Ω—ã–π type. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å 'country' –∏–ª–∏ 'category'"}, 400)
                return

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_file = self.base_dir / "metadata_config.json"
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            else:
                metadata = {"countries": [], "categories": []}

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
            list_key = "countries" if metadata_type == "country" else "categories"
            if value not in metadata[list_key]:
                metadata[list_key].append(value)
                metadata[list_key].sort()  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)

                self.send_json_response({"success": True, "message": f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {value}"})
            else:
                self.send_json_response({"success": False, "message": f"{value} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"})

        except Exception as e:
            print(f"Error adding metadata: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_remove_metadata(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            metadata_type = data.get("type")  # "country" –∏–ª–∏ "category"
            value = data.get("value", "").strip()

            if not metadata_type or not value:
                self.send_json_response({"error": "–ù–µ —É–∫–∞–∑–∞–Ω type –∏–ª–∏ value"}, 400)
                return

            if metadata_type not in ["country", "category"]:
                self.send_json_response({"error": "–ù–µ–≤–µ—Ä–Ω—ã–π type. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å 'country' –∏–ª–∏ 'category'"}, 400)
                return

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_file = self.base_dir / "metadata_config.json"
            if not metadata_file.exists():
                self.send_json_response({"error": "–§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω"}, 404)
                return

            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            # –£–¥–∞–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            list_key = "countries" if metadata_type == "country" else "categories"
            if value in metadata[list_key]:
                metadata[list_key].remove(value)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)

                self.send_json_response({"success": True, "message": f"–£–¥–∞–ª–µ–Ω–æ: {value}"})
            else:
                self.send_json_response({"success": False, "message": f"{value} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"})

        except Exception as e:
            print(f"Error removing metadata: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            countries = data.get("countries", [])
            categories = data.get("categories", [])

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            if not isinstance(countries, list) or not isinstance(categories, list):
                self.send_json_response({"error": "Invalid data format"}, 400)
                return

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_file = self.base_dir / "metadata_config.json"
            metadata = {
                "countries": countries,
                "categories": categories
            }

            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            self.send_json_response({"success": True, "message": "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã"})

        except Exception as e:
            print(f"Error saving metadata: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_metadata_search(self):
        """–ü–æ–∏—Å–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö email —á–µ—Ä–µ–∑ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            from metadata_database import MetadataDatabase

            query_params = parse_qs(urlparse(self.path).query)

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            email_pattern = query_params.get('email', [''])[0]
            country = query_params.get('country', [''])[0]
            category = query_params.get('category', [''])[0]
            validation_status_param = query_params.get('validation_status', [''])[0]
            has_phone = query_params.get('has_phone', [''])[0]
            source_file = query_params.get('source_file', [''])[0]
            country_mismatch = query_params.get('country_mismatch', [''])[0]
            limit = min(int(query_params.get('limit', [5000])[0]), 10000)  # –£–≤–µ–ª–∏—á–µ–Ω –¥–æ 10000
            offset = int(query_params.get('offset', [0])[0])

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º validation_status –≤ —Å–ø–∏—Å–æ–∫ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞)
            validation_statuses = None
            if validation_status_param:
                validation_statuses = [s.strip() for s in validation_status_param.split(',') if s.strip()]

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º has_phone –≤ boolean
            has_phone_bool = None
            if has_phone.lower() == 'true':
                has_phone_bool = True
            elif has_phone.lower() == 'false':
                has_phone_bool = False

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º country_mismatch –≤ int
            country_mismatch_int = None
            if country_mismatch:
                country_mismatch_int = int(country_mismatch)

            with MetadataDatabase() as db:
                # –ü–æ–∏—Å–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                results = db.search_metadata(
                    email_pattern=email_pattern if email_pattern else None,
                    country=country if country else None,
                    category=category if category else None,
                    validation_statuses=validation_statuses,
                    has_phone=has_phone_bool,
                    source_file=source_file if source_file else None,
                    country_mismatch=country_mismatch_int,
                    limit=limit,
                    offset=offset
                )

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
                emails_data = []
                for metadata in results:
                    email_dict = {
                        "email": metadata.email,
                        "domain": metadata.domain,
                        "source_url": metadata.source_url,
                        "page_title": metadata.page_title,
                        "company_name": metadata.company_name,
                        "phone": metadata.phone,
                        "country": metadata.country,
                        "city": metadata.city,
                        "address": metadata.address,
                        "category": metadata.category,
                        "keywords": metadata.keywords,
                        "meta_description": metadata.meta_description,
                        "meta_keywords": metadata.meta_keywords,
                        "validation_status": metadata.validation_status,
                        "validation_date": metadata.validation_date,
                        "source_file": metadata.source_file,
                        "list_country": metadata.list_country,
                        "country_mismatch": metadata.country_mismatch
                    }
                    emails_data.append(email_dict)

                response = {
                    "emails": emails_data,
                    "count": len(emails_data),
                    "offset": offset,
                    "limit": limit
                }

                self.send_json_response(response)

        except Exception as e:
            import traceback
            print(f"Error searching metadata: {e}")
            print(traceback.format_exc())
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_metadata_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º"""
        try:
            from metadata_database import MetadataDatabase

            with MetadataDatabase() as db:
                stats = db.get_statistics()
                self.send_json_response(stats)

        except Exception as e:
            print(f"Error getting metadata stats: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_batch_update_status(self):
        """–ú–∞—Å—Å–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ emails"""
        try:
            from metadata_database import MetadataDatabase

            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            emails = data.get("emails", [])
            new_status = data.get("status", "")

            if not emails:
                self.send_json_response({"success": False, "message": "–°–ø–∏—Å–æ–∫ emails –ø—É—Å—Ç"}, 400)
                return

            if new_status not in ['Valid', 'NotSure', 'Temp', 'Invalid']:
                self.send_json_response({"success": False, "message": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Å—Ç–∞—Ç—É—Å"}, 400)
                return

            with MetadataDatabase() as db:
                success, updated_count = db.batch_update_validation_status(emails, new_status)

                if success:
                    self.send_json_response({
                        "success": True,
                        "message": f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø–∏—Å–µ–π",
                        "updated_count": updated_count
                    })
                else:
                    self.send_json_response({
                        "success": False,
                        "message": "–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"
                    }, 500)

        except Exception as e:
            print(f"Error batch updating status: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_batch_update_field(self):
        """–ú–∞—Å—Å–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è –¥–ª—è —Å–ø–∏—Å–∫–∞ emails"""
        try:
            from metadata_database import MetadataDatabase

            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            emails = data.get("emails", [])
            field_name = data.get("field", "")
            new_value = data.get("value", "")

            if not emails:
                self.send_json_response({"success": False, "message": "–°–ø–∏—Å–æ–∫ emails –ø—É—Å—Ç"}, 400)
                return

            if not field_name:
                self.send_json_response({"success": False, "message": "–ù–µ —É–∫–∞–∑–∞–Ω–æ –ø–æ–ª–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"}, 400)
                return

            with MetadataDatabase() as db:
                success, updated_count = db.batch_update_field(emails, field_name, new_value)

                if success:
                    self.send_json_response({
                        "success": True,
                        "message": f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø–∏—Å–µ–π",
                        "updated_count": updated_count
                    })
                else:
                    self.send_json_response({
                        "success": False,
                        "message": "–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"
                    }, 500)

        except Exception as e:
            print(f"Error batch updating field: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_country_mismatches(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ emails —Å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º —Å—Ç—Ä–∞–Ω"""
        try:
            from metadata_database import MetadataDatabase

            query_params = parse_qs(urlparse(self.path).query)
            source_file = query_params.get('source_file', [''])[0]
            limit = min(int(query_params.get('limit', [1000])[0]), 10000)

            with MetadataDatabase() as db:
                mismatches = db.get_country_mismatches(
                    source_file=source_file if source_file else None,
                    limit=limit
                )

                emails_data = []
                for metadata in mismatches:
                    email_dict = {
                        "email": metadata.email,
                        "domain": metadata.domain,
                        "country": metadata.country,
                        "list_country": metadata.list_country,
                        "country_mismatch": metadata.country_mismatch,
                        "company_name": metadata.company_name,
                        "phone": metadata.phone,
                        "category": metadata.category,
                        "validation_status": metadata.validation_status,
                        "source_file": metadata.source_file
                    }
                    emails_data.append(email_dict)

                response = {
                    "emails": emails_data,
                    "count": len(emails_data)
                }

                self.send_json_response(response)

        except Exception as e:
            print(f"Error getting country mismatches: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_lvp_sources(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö LVP —Ñ–∞–π–ª–æ–≤"""
        try:
            from metadata_database import MetadataDatabase

            with MetadataDatabase() as db:
                sources = db.get_lvp_sources()
                sources_data = []
                for source in sources:
                    source_dict = {
                        "filename": source.filename,
                        "file_path": source.file_path,
                        "import_date": source.import_date,
                        "total_emails": source.total_emails,
                        "valid_emails": source.valid_emails,
                        "invalid_emails": source.invalid_emails,
                        "file_size": source.file_size
                    }
                    sources_data.append(source_dict)

                self.send_json_response({"sources": sources_data})

        except Exception as e:
            print(f"Error getting LVP sources: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_import_lvp(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ LVP —Ñ–∞–π–ª–æ–≤"""
        try:
            from lvp_importer import LVPImporter

            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            action = data.get("action")

            if action == "scan":
                # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                with LVPImporter() as importer:
                    suggestions = importer.get_import_suggestions()
                    self.send_json_response(suggestions)

            elif action == "import":
                # –ò–º–ø–æ—Ä—Ç —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                file_paths = data.get("file_paths", [])
                if not file_paths:
                    self.send_json_response({"error": "–ù–µ —É–∫–∞–∑–∞–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞"}, 400)
                    return

                with LVPImporter() as importer:
                    result = importer.import_multiple_lvp_files(file_paths)
                    self.send_json_response(result)

            elif action == "import_single":
                # –ò–º–ø–æ—Ä—Ç –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                file_path = data.get("file_path")
                if not file_path:
                    self.send_json_response({"error": "–ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞"}, 400)
                    return

                print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–º–ø–æ—Ä—Ç —Ñ–∞–π–ª–∞: {file_path}")
                with LVPImporter() as importer:
                    result = importer.import_lvp_file(file_path)
                    print(f"‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º: {result.get('success', False)}")
                    self.send_json_response(result)

            elif action == "import_all":
                # –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
                with LVPImporter() as importer:
                    new_files = importer.scan_downloads_folder()
                    if new_files:
                        result = importer.import_multiple_lvp_files(new_files)
                        self.send_json_response(result)
                    else:
                        self.send_json_response({
                            "success": False,
                            "message": "–ù–µ—Ç –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞"
                        })

            else:
                self.send_json_response({"error": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ"}, 400)

        except Exception as e:
            print(f"Error handling LVP import: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_export_lvp(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ LVP —Ñ–æ—Ä–º–∞—Ç"""
        try:
            from lvp_exporter import LVPExporter

            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
            filters = {}

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
            if data.get('country'):
                filters['country'] = data['country']
            if data.get('validation_status'):
                filters['validation_status'] = data['validation_status']
            if data.get('source_file'):
                filters['source_file'] = data['source_file']
            if data.get('category'):
                filters['category'] = data['category']
            if data.get('has_phone') is not None:
                filters['has_phone'] = data['has_phone']
            if data.get('country_mismatch') is not None:
                filters['country_mismatch'] = data['country_mismatch']

            # –õ–∏–º–∏—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000000)
            limit = data.get('limit', 1000000)
            if limit:
                filters['limit'] = min(int(limit), 1000000)  # –ú–∞–∫—Å–∏–º—É–º 1M

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"metadata_export_{timestamp}.lvp"
            output_path = self.base_dir / "output" / filename

            print(f"\nüì§ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–æ—Ä—Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ LVP")
            print(f"üìä –§–∏–ª—å—Ç—Ä—ã: {filters}")
            print(f"üìä –õ–∏–º–∏—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞: {limit}")
            print(f"üìÅ –§–∞–π–ª: {output_path}")

            # –í—ã–ø–æ–ª–Ω—è–µ–º —ç–∫—Å–ø–æ—Ä—Ç
            with LVPExporter() as exporter:
                if filters:
                    # –≠–∫—Å–ø–æ—Ä—Ç —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
                    result = exporter.export_filtered_metadata(str(output_path), filters)
                else:
                    # –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                    result = exporter.export_all_metadata(str(output_path), limit=limit)

                if result['success']:
                    # –î–æ–±–∞–≤–ª—è–µ–º URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                    result['download_url'] = f"/output/{filename}"
                    result['filename'] = filename

                    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ: {result['total_exported']} –∑–∞–ø–∏—Å–µ–π")

                    self.send_json_response(result)
                else:
                    self.send_json_response({
                        "success": False,
                        "error": result.get('error', 'Unknown error')
                    }, 400)

        except Exception as e:
            import traceback
            print(f"Error handling LVP export: {e}")
            print(traceback.format_exc())
            self.send_json_response({"error": str(e)}, 500)

    def handle_enrich_list(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
        try:
            from email_enricher import EmailEnricher

            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            filename = data.get("filename")
            force_overwrite = data.get("force_overwrite", False)

            if not filename:
                self.send_json_response({"error": "–ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è"}, 400)
                return

            with EmailEnricher() as enricher:
                result = enricher.enrich_email_list(filename, force_overwrite)

                if result["success"]:
                    self.send_json_response({
                        "success": True,
                        "message": f"–°–ø–∏—Å–æ–∫ {result['input_file']} —É—Å–ø–µ—à–Ω–æ –æ–±–æ–≥–∞—â–µ–Ω",
                        "result": result
                    })
                else:
                    self.send_json_response({
                        "success": False,
                        "error": result["error"]
                    }, 400)

        except Exception as e:
            print(f"Error enriching list: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_enrich_all(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤"""
        try:
            from email_enricher import EmailEnricher

            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            force_overwrite = data.get("force_overwrite", False)

            with EmailEnricher() as enricher:
                result = enricher.enrich_all_lists(force_overwrite)
                self.send_json_response(result)

        except Exception as e:
            print(f"Error enriching all lists: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_all_files(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –í–°–ï–• —Ñ–∞–π–ª–æ–≤ –∏–∑ input/ –∏ output/ –¥–ª—è –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏"""
        try:
            input_dir = self.base_dir / "input"
            output_dir = self.base_dir / "output"

            files = []

            # –°–∫–∞–Ω–∏—Ä—É–µ–º input/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            if input_dir.exists():
                for file_path in input_dir.iterdir():
                    if file_path.is_file():
                        try:
                            stat = file_path.stat()
                            files.append({
                                "name": file_path.name,
                                "path": str(file_path.relative_to(self.base_dir)),
                                "size": stat.st_size,
                                "modified": stat.st_mtime
                            })
                        except Exception as e:
                            print(f"Error reading file {file_path}: {e}")

            # –°–∫–∞–Ω–∏—Ä—É–µ–º output/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            if output_dir.exists():
                for file_path in output_dir.iterdir():
                    if file_path.is_file():
                        try:
                            stat = file_path.stat()
                            files.append({
                                "name": file_path.name,
                                "path": str(file_path.relative_to(self.base_dir)),
                                "size": stat.st_size,
                                "modified": stat.st_mtime
                            })
                        except Exception as e:
                            print(f"Error reading file {file_path}: {e}")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
            files.sort(key=lambda x: x["modified"], reverse=True)

            self.send_json_response({"files": files})

        except Exception as e:
            print(f"Error getting all files: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    # ===== ADMIN HANDLERS =====

    def handle_admin_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            import shutil
            import sqlite3

            stats = {}

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞
            cache_file = self.base_dir / ".cache" / "processed_files.json"
            if cache_file.exists():
                cache_size = cache_file.stat().st_size
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                stats["cache"] = {
                    "exists": True,
                    "size": cache_size,
                    "files_cached": len(cache_data),
                    "path": str(cache_file)
                }
            else:
                stats["cache"] = {"exists": False}

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            db_file = self.base_dir / "metadata.db"
            if db_file.exists():
                db_size = db_file.stat().st_size
                try:
                    conn = sqlite3.connect(str(db_file))
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM emails")
                    email_count = cursor.fetchone()[0]
                    conn.close()

                    stats["database"] = {
                        "exists": True,
                        "size": db_size,
                        "email_count": email_count,
                        "path": str(db_file)
                    }
                except Exception as e:
                    stats["database"] = {
                        "exists": True,
                        "size": db_size,
                        "error": str(e)
                    }
            else:
                stats["database"] = {"exists": False}

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            input_dir = self.base_dir / "input"
            output_dir = self.base_dir / "output"

            stats["directories"] = {
                "input": {
                    "files": len(list(input_dir.glob("*"))) if input_dir.exists() else 0,
                    "size": sum(f.stat().st_size for f in input_dir.glob("*") if f.is_file()) if input_dir.exists() else 0
                },
                "output": {
                    "files": len(list(output_dir.glob("*"))) if output_dir.exists() else 0,
                    "size": sum(f.stat().st_size for f in output_dir.glob("*") if f.is_file()) if output_dir.exists() else 0
                }
            }

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
            disk = shutil.disk_usage(str(self.base_dir))
            stats["disk"] = {
                "total": disk.total,
                "used": disk.used,
                "free": disk.free,
                "percent_used": round((disk.used / disk.total) * 100, 2)
            }

            self.send_json_response({"stats": stats})

        except Exception as e:
            print(f"Error getting admin stats: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def _calculate_country_stats(self):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç—Ä–∞–Ω–∞–º: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ email –∏ –∫–∞—á–µ—Å—Ç–≤–æ
        Returns: list of dicts —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–µ
        """
        country_stats = {}

        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º lists_config.json –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ñ–∞–π–ª ‚Üí —Å—Ç—Ä–∞–Ω–∞
            config_file = self.base_dir / "lists_config.json"
            if not config_file.exists():
                return []

            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                lists = config.get("lists", [])

            # 2. –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥: stem —Ñ–∞–π–ª–∞ ‚Üí —Å—Ç—Ä–∞–Ω–∞
            file_to_country = {}
            for lst in lists:
                filename = lst.get("filename", "")
                country = lst.get("country", "Unknown")

                if not filename or country == "Unknown":
                    continue

                # –ü–æ–ª—É—á–∞–µ–º stem (–∏–º—è –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
                stem = Path(filename).stem
                file_to_country[stem] = country

            # 3. –°–∫–∞–Ω–∏—Ä—É–µ–º output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            output_dir = self.base_dir / "output"
            if not output_dir.exists():
                return []

            # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º clean –∏ blocked —Ñ–∞–π–ª—ã
            clean_files = list(output_dir.glob("*_clean_*.txt"))
            blocked_files = list(output_dir.glob("*_blocked_*.txt"))

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º clean —Ñ–∞–π–ª—ã
            for f in clean_files:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º stem –∏–∑ –∏–º–µ–Ω–∏ output —Ñ–∞–π–ª–∞
                # –ü—Ä–∏–º–µ—Ä: "Italy_Agriculture_clean_20251024.txt" ‚Üí "Italy_Agriculture"
                filename = f.name

                # –ò—â–µ–º stem –≤ –Ω–∞—á–∞–ª–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                matched_stem = None
                for stem in file_to_country.keys():
                    if filename.startswith(stem):
                        matched_stem = stem
                        break

                if not matched_stem:
                    continue

                country = file_to_country[matched_stem]

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —Å—Ç—Ä–∞–Ω—ã
                if country not in country_stats:
                    country_stats[country] = {
                        "country": country,
                        "clean_emails": 0,
                        "blocked_emails": 0,
                        "invalid_emails": 0
                    }

                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º email
                try:
                    with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                        count = sum(1 for line in file if line.strip() and '@' in line)
                        country_stats[country]["clean_emails"] += count
                except Exception as e:
                    print(f"Error reading {f.name}: {e}")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º blocked —Ñ–∞–π–ª—ã
            for f in blocked_files:
                filename = f.name

                matched_stem = None
                for stem in file_to_country.keys():
                    if filename.startswith(stem):
                        matched_stem = stem
                        break

                if not matched_stem:
                    continue

                country = file_to_country[matched_stem]

                if country not in country_stats:
                    country_stats[country] = {
                        "country": country,
                        "clean_emails": 0,
                        "blocked_emails": 0,
                        "invalid_emails": 0
                    }

                try:
                    with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                        count = sum(1 for line in file if line.strip() and '@' in line)
                        country_stats[country]["blocked_emails"] += count
                except Exception as e:
                    print(f"Error reading {f.name}: {e}")

            # 5. –í—ã—á–∏—Å–ª—è–µ–º total_emails –∏ quality_score –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω—ã
            result = []
            for country, data in country_stats.items():
                total = data["clean_emails"] + data["blocked_emails"]
                quality_score = 0

                if total > 0:
                    quality_score = (data["clean_emails"] / total) * 100

                result.append({
                    "country": country,
                    "clean_emails": data["clean_emails"],
                    "blocked_emails": data["blocked_emails"],
                    "total_emails": total,
                    "quality_score": round(quality_score, 1)
                })

            # 6. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ total_emails (—É–±—ã–≤–∞–Ω–∏–µ)
            result.sort(key=lambda x: x["total_emails"], reverse=True)

            return result

        except Exception as e:
            import traceback
            print(f"Error calculating country stats: {e}")
            print(traceback.format_exc())
            return []

    def handle_get_dashboard_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞ –∏–∑ –ë–î (–ë–´–°–¢–†–û)"""
        try:
            cache_db_path = self.base_dir / ".cache" / "processing_cache_optimized.db"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ë–î
            if not cache_db_path.exists():
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
                print("‚ö†Ô∏è  –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥")
                return self._handle_get_dashboard_stats_fallback()

            conn = sqlite3.connect(cache_db_path)
            cursor = conn.cursor()

            stats = {
                "total_lists": 0,
                "processed_emails": 0,
                "clean_emails": 0,
                "blocked_emails": 0,
                "invalid_emails": 0,
                "countries": [],
                "categories": {},
                "recent_activity": [],
                "queue_length": 0,
                "country_stats": []  # Frontend –æ–∂–∏–¥–∞–µ—Ç –º–∞—Å—Å–∏–≤!
            }

            # 1. –ü–æ–ª—É—á–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (1 –∑–∞–ø—Ä–æ—Å, <5ms)
            try:
                cursor.execute('''
                    SELECT
                        total_lists,
                        total_processed_emails,
                        total_clean_emails,
                        total_blocked_emails,
                        total_invalid_emails,
                        countries_json,
                        categories_json,
                        last_updated
                    FROM processing_statistics
                    WHERE id = 1
                ''')

                row = cursor.fetchone()

                if row:
                    stats["total_lists"] = row[0] or 0
                    stats["processed_emails"] = row[1] or 0
                    stats["clean_emails"] = row[2] or 0
                    stats["blocked_emails"] = row[3] or 0
                    stats["invalid_emails"] = row[4] or 0
                    stats["countries"] = json.loads(row[5] or "[]")
                    stats["categories"] = json.loads(row[6] or "{}")
                    stats["last_updated"] = row[7]
                else:
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—É—Å—Ç–∞, —Å—á–∏—Ç–∞–µ–º –∏–∑ processed_files
                    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—É—Å—Ç–∞, —Å—á–∏—Ç–∞–µ–º –∏–∑ processed_files...")
                    cursor.execute('''
                        SELECT
                            COUNT(DISTINCT filename) as total_lists,
                            COALESCE(SUM(total_count), 0) as processed,
                            COALESCE(SUM(clean_count), 0) as clean,
                            COALESCE(SUM(blocked_count), 0) as blocked,
                            COALESCE(SUM(invalid_count), 0) as invalid
                        FROM processed_files
                        WHERE file_hash IS NOT NULL
                    ''')

                    fallback = cursor.fetchone()
                    if fallback:
                        stats["total_lists"] = fallback[0] or 0
                        stats["processed_emails"] = fallback[1] or 0
                        stats["clean_emails"] = fallback[2] or 0
                        stats["blocked_emails"] = fallback[3] or 0
                        stats["invalid_emails"] = fallback[4] or 0

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

            # 2. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç—Ä–∞–Ω–∞–º (1 –∑–∞–ø—Ä–æ—Å, <10ms)
            try:
                cursor.execute('''
                    SELECT
                        country,
                        clean_emails,
                        blocked_emails,
                        total_emails,
                        quality_score
                    FROM country_statistics
                    ORDER BY total_emails DESC
                    LIMIT 20
                ''')

                # –í–ê–ñ–ù–û: Frontend –æ–∂–∏–¥–∞–µ—Ç –º–∞—Å—Å–∏–≤, –∞ –Ω–µ –æ–±—ä–µ–∫—Ç!
                country_list = []
                for row in cursor.fetchall():
                    country_list.append({
                        "country": row[0],
                        "clean_emails": row[1] or 0,
                        "blocked_emails": row[2] or 0,
                        "total": row[3] or 0,
                        "quality_score": round(row[4] or 0.0, 2)
                    })

                stats["country_stats"] = country_list

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º: {e}")
                stats["country_stats"] = []

            # 3. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (1 –∑–∞–ø—Ä–æ—Å, <5ms)
            try:
                cursor.execute('''
                    SELECT
                        filename,
                        processed_at,
                        total_emails,
                        clean_emails,
                        blocked_emails,
                        output_size
                    FROM processing_history
                    WHERE success = 1
                    ORDER BY processed_at DESC
                    LIMIT 10
                ''')

                for row in cursor.fetchall():
                    stats["recent_activity"].append({
                        "filename": row[0],
                        "processed_at": row[1],
                        "total_emails": row[2] or 0,
                        "clean": row[3] or 0,
                        "blocked": row[4] or 0,
                        "size": row[5] or 0
                    })

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

            # 4. –î–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏ (–∏–∑ –ø–∞–º—è—Ç–∏)
            if hasattr(self, 'processing_state') and self.processing_state:
                stats["queue_length"] = len([
                    p for p in self.processing_state.values()
                    if p.get("status") == "processing"
                ])

            conn.close()

            self.send_json_response({"stats": stats})

        except Exception as e:
            import traceback
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—à–±–æ—Ä–¥–∞: {e}")
            print(traceback.format_exc())
            self.send_json_response({"error": str(e)}, 500)

    def _handle_get_dashboard_stats_fallback(self):
        """Fallback –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–æ–≤ (–º–µ–¥–ª–µ–Ω–Ω—ã–π, –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        try:
            stats = {
                "total_lists": 0,
                "processed_emails": 0,
                "clean_emails": 0,
                "blocked_emails": 0,
                "invalid_emails": 0,
                "countries": [],
                "categories": {},
                "recent_activity": []
            }

            # –ß–∏—Ç–∞–µ–º lists_config.json
            config_file = self.base_dir / "lists_config.json"
            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        lists = config.get("lists", [])
                        stats["total_lists"] = len(lists)

                        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        countries_set = set()
                        for lst in lists:
                            country = lst.get("country", "Unknown")
                            if country and country != "Unknown":
                                countries_set.add(country)

                            category = lst.get("category", "General")
                            if category:
                                stats["categories"][category] = stats["categories"].get(category, 0) + 1

                        stats["countries"] = sorted(list(countries_set))
                except Exception as e:
                    print(f"Error reading lists config: {e}")

            # –°—á–∏—Ç–∞–µ–º email –∏–∑ output —Ñ–∞–π–ª–æ–≤
            output_dir = self.base_dir / "output"
            if output_dir.exists():
                try:
                    # –ü–æ–¥—Å—á–µ—Ç clean emails
                    clean_files = list(output_dir.glob("*_clean_*.txt"))
                    for f in clean_files:
                        try:
                            with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                                count = sum(1 for line in file if line.strip() and '@' in line)
                                stats["clean_emails"] += count
                        except Exception as e:
                            print(f"Error reading clean file {f.name}: {e}")

                    # –ü–æ–¥—Å—á–µ—Ç blocked emails
                    blocked_files = list(output_dir.glob("*_blocked_*.txt"))
                    for f in blocked_files:
                        try:
                            with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                                count = sum(1 for line in file if line.strip() and '@' in line)
                                stats["blocked_emails"] += count
                        except Exception as e:
                            print(f"Error reading blocked file {f.name}: {e}")

                    # –ü–æ–¥—Å—á–µ—Ç invalid emails
                    invalid_files = list(output_dir.glob("*_invalid_*.txt"))
                    for f in invalid_files:
                        try:
                            with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                                count = sum(1 for line in file if line.strip())
                                stats["invalid_emails"] += count
                        except Exception as e:
                            print(f"Error reading invalid file {f.name}: {e}")

                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ –¥–∞—Ç–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    all_output_files = list(output_dir.glob("*_clean_*.txt")) + list(output_dir.glob("*_blocked_*.txt"))
                    if all_output_files:
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
                        all_output_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

                        for f in all_output_files[:10]:
                            stats["recent_activity"].append({
                                "filename": f.name,
                                "size": f.stat().st_size,
                                "modified": f.stat().st_mtime
                            })

                except Exception as e:
                    print(f"Error processing output directory: {e}")

            stats["processed_emails"] = stats["clean_emails"] + stats["blocked_emails"]

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—á–µ—Ä–µ–¥–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏)
            stats["queue_length"] = 0
            if hasattr(self, 'processing_state') and self.processing_state:
                stats["queue_length"] = len([p for p in self.processing_state.values() if p.get("status") == "processing"])

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç—Ä–∞–Ω–∞–º (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –∫–∞—á–µ—Å—Ç–≤–æ)
            country_stats = self._calculate_country_stats()
            stats["country_stats"] = country_stats

            self.send_json_response({"stats": stats})

        except Exception as e:
            import traceback
            print(f"Error getting dashboard stats: {e}")
            print(traceback.format_exc())
            self.send_json_response({"error": str(e)}, 500)

    def handle_admin_clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            cache_file = self.base_dir / ".cache" / "processed_files.json"

            if not cache_file.exists():
                self.send_json_response({"success": True, "message": "–ö–µ—à –Ω–µ –Ω–∞–π–¥–µ–Ω"})
                return

            # –£–¥–∞–ª—è–µ–º –∫–µ—à —Ñ–∞–π–ª
            cache_file.unlink()

            self.send_json_response({
                "success": True,
                "message": "–ö–µ—à —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω"
            })

        except Exception as e:
            print(f"Error clearing cache: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_admin_optimize_db(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            import sqlite3

            db_file = self.base_dir / "metadata.db"

            if not db_file.exists():
                self.send_json_response({"success": False, "error": "–ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}, 404)
                return

            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            size_before = db_file.stat().st_size

            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ë–î
            conn = sqlite3.connect(str(db_file))
            conn.execute("VACUUM")
            conn.close()

            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            size_after = db_file.stat().st_size
            saved = size_before - size_after

            self.send_json_response({
                "success": True,
                "message": "–ë–î —É—Å–ø–µ—à–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞",
                "size_before": size_before,
                "size_after": size_after,
                "saved": saved,
                "percent_saved": round((saved / size_before) * 100, 2) if size_before > 0 else 0
            })

        except Exception as e:
            print(f"Error optimizing database: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_admin_delete_file(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ input/ –∏–ª–∏ output/"""
        try:
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length == 0:
                self.send_json_response({"error": "No data provided"}, 400)
                return

            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            file_path = data.get("path", "").strip()
            if not file_path:
                self.send_json_response({"error": "File path is required"}, 400)
                return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏
            safe_path = Path(file_path)
            if safe_path.is_absolute():
                self.send_json_response({"error": "Absolute paths not allowed"}, 400)
                return

            full_path = (self.base_dir / safe_path).resolve()
            base_path = self.base_dir.resolve()

            if not str(full_path).startswith(str(base_path)):
                self.send_json_response({"error": "Path traversal attempt"}, 400)
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
            input_dir = self.base_dir / "input"
            output_dir = self.base_dir / "output"

            in_input = str(full_path).startswith(str(input_dir.resolve()))
            in_output = str(full_path).startswith(str(output_dir.resolve()))

            if not (in_input or in_output):
                self.send_json_response({"error": "File must be in input/ or output/"}, 403)
                return

            if not full_path.exists():
                self.send_json_response({"error": "File not found"}, 404)
                return

            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            full_path.unlink()

            # –ï—Å–ª–∏ —Ñ–∞–π–ª –≤ input/, –æ–±–Ω–æ–≤–ª—è–µ–º lists_config.json
            if in_input:
                filename = full_path.name
                config_file = self.base_dir / "lists_config.json"

                if config_file.exists():
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)

                    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    lists = [lst for lst in config.get("lists", []) if lst.get("filename") != filename]
                    config["lists"] = lists

                    with open(config_file, 'w', encoding='utf-8') as f:
                        json.dump(config, f, ensure_ascii=False, indent=2)

            self.send_json_response({
                "success": True,
                "message": f"–§–∞–π–ª {full_path.name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω"
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error deleting file: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_admin_reset_system(self):
        """–°–±—Ä–æ—Å —Å–∏—Å—Ç–µ–º—ã —á–µ—Ä–µ–∑ reset_system.py"""
        global processing_state

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–¥–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
            with processing_state["lock"]:
                if processing_state["is_running"]:
                    self.send_json_response({
                        "success": False,
                        "error": "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–±—Ä–æ—Å –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
                    }, 409)
                    return

            content_length = int(self.headers.get('Content-Length', 0))
            if content_length == 0:
                self.send_json_response({"error": "No data provided"}, 400)
                return

            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–±—Ä–æ—Å–∞
            clean_cache = data.get("clean_cache", False)
            clean_config = data.get("clean_config", False)
            clean_output = data.get("clean_output", False)
            clean_metadata_db = data.get("clean_metadata_db", False)
            full_reset = data.get("full_reset", False)
            backup = data.get("backup", True)

            if not any([clean_cache, clean_config, clean_output, clean_metadata_db, full_reset]):
                self.send_json_response({"error": "–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –æ–ø—Ü–∏—é —Å–±—Ä–æ—Å–∞"}, 400)
                return

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É
            import sys
            cmd = [sys.executable, "reset_system.py"]

            if full_reset:
                cmd.append("--full-reset")
            else:
                if clean_cache:
                    cmd.append("--clean-cache")
                if clean_config:
                    cmd.append("--clean-config")
                if clean_output:
                    cmd.append("--clean-output")
                if clean_metadata_db:
                    cmd.append("--clean-metadata-db")

            if not backup:
                cmd.append("--no-backup")

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ
            def run_reset():
                import subprocess
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
                print(f"Reset system output: {result.stdout}")
                if result.stderr:
                    print(f"Reset system errors: {result.stderr}")

            import threading
            thread = threading.Thread(target=run_reset)
            thread.daemon = True
            thread.start()

            self.send_json_response({
                "success": True,
                "message": "–°–±—Ä–æ—Å —Å–∏—Å—Ç–µ–º—ã –∑–∞–ø—É—â–µ–Ω"
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error resetting system: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_admin_restore_data(self):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ restore_data.py"""
        global processing_state

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–¥–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
            with processing_state["lock"]:
                if processing_state["is_running"]:
                    self.send_json_response({
                        "success": False,
                        "error": "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
                    }, 409)
                    return

            content_length = int(self.headers.get('Content-Length', 0))
            if content_length == 0:
                self.send_json_response({"error": "No data provided"}, 400)
                return

            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode())

            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            mode = data.get("mode", "all")  # all, step1, step2, step3, step4
            unified = data.get("unified", True)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É
            import sys
            cmd = [sys.executable, "restore_data.py"]

            if mode == "all":
                cmd.append("--all")
                if unified:
                    cmd.append("--unified")
            elif mode == "step1":
                cmd.append("--step1")
            elif mode == "step2":
                cmd.append("--step2")
            elif mode == "step3":
                cmd.append("--step3")
            elif mode == "step4":
                cmd.append("--step4")
            else:
                self.send_json_response({"error": f"Invalid mode: {mode}"}, 400)
                return

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            def run_restore():
                global processing_state

                with processing_state["lock"]:
                    processing_state["is_running"] = True
                    processing_state["logs"].clear()
                    processing_state["start_time"] = datetime.now()

                import subprocess
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,
                    cwd=str(self.base_dir)
                )

                # –ß–∏—Ç–∞–µ–º –≤—ã–≤–æ–¥ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
                for line in process.stdout:
                    with processing_state["lock"]:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": line.strip()
                        })

                process.wait()

                with processing_state["lock"]:
                    processing_state["is_running"] = False
                    if process.returncode == 0:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": "‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ"
                        })
                    else:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ö†Ô∏è –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —Å –∫–æ–¥–æ–º: {process.returncode}"
                        })

            import threading
            thread = threading.Thread(target=run_restore)
            thread.daemon = True
            thread.start()

            message = f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—â–µ–Ω–æ (—Ä–µ–∂–∏–º: {mode}"
            if mode == "all" and unified:
                message += ", –µ–¥–∏–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞"
            message += ")"

            self.send_json_response({
                "success": True,
                "message": message
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error restoring data: {e}")
            self.send_json_response({"error": str(e)}, 500)

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç—Ä–µ–±—É—é—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –∑–∞—â–∏—Ç—ã...
    # –≠—Ç–æ –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ —É—è–∑–≤–∏–º–æ—Å—Ç—è–º–∏

    def handle_get_available_smart_filters(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —É–º–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
        try:
            from smart_filters import list_available_filters

            filters = list_available_filters()

            self.send_json_response({
                "success": True,
                "filters": filters
            })

        except Exception as e:
            print(f"Error getting available smart filters: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_smart_filter_config(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —É–º–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞"""
        try:
            # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ query string
            from urllib.parse import parse_qs
            query = urlparse(self.path).query
            params = parse_qs(query)

            filter_name = params.get('name', ['italy_hydraulics'])[0]

            from smart_filters import get_config_path
            import json

            config_path = get_config_path(filter_name)
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            self.send_json_response({
                "success": True,
                "config": config
            })

        except FileNotFoundError:
            self.send_json_response({"error": f"Config not found for filter: {filter_name}"}, 404)
        except Exception as e:
            print(f"Error getting smart filter config: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_auto_suggest_config(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        try:
            # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ query string
            from urllib.parse import parse_qs
            query = urlparse(self.path).query
            params = parse_qs(query)

            filename = params.get('filename', [''])[0]

            if not filename:
                self.send_json_response({"error": "Missing filename parameter"}, 400)
                return

            from smart_filters import auto_suggest_config

            suggested_config = auto_suggest_config(filename)

            if suggested_config:
                self.send_json_response({
                    "success": True,
                    "suggestion": suggested_config,
                    "message": f"Suggested config: {suggested_config['name']} (confidence: {suggested_config.get('confidence', 'unknown')})"
                })
            else:
                self.send_json_response({
                    "success": True,
                    "suggestion": None,
                    "message": "No suitable config found. Please select manually or create new config."
                })

        except Exception as e:
            print(f"Error in auto-suggest: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_smart_filter_process(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ clean-—Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length > 1024 * 1024:  # 1MB max
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length).decode('utf-8')
            data = json.loads(post_data)

            clean_file = data.get('clean_file')
            filter_name = data.get('filter_name', 'italy_hydraulics')
            include_metadata = data.get('include_metadata', True)

            if not clean_file:
                self.send_json_response({"error": "Missing clean_file parameter"}, 400)
                return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            from pathlib import Path
            validate_filename(clean_file)

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
            def run_smart_filter():
                try:
                    from smart_filter_processor import SmartFilterProcessor

                    processor = SmartFilterProcessor(filter_name=filter_name)
                    result = processor.process_clean_file(
                        Path(clean_file),
                        include_metadata=include_metadata
                    )

                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚úÖ Smart filter –∑–∞–≤–µ—Ä—à–µ–Ω: {result.stats}"
                        })

                except Exception as error:
                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ùå –û—à–∏–±–∫–∞ smart filter: {str(error)}"
                        })

            import threading
            thread = threading.Thread(target=run_smart_filter)
            thread.daemon = True
            thread.start()

            with processing_state["lock"]:
                processing_state["is_running"] = True
                processing_state["start_time"] = datetime.now()

            self.send_json_response({
                "success": True,
                "message": f"Smart filter –∑–∞–ø—É—â–µ–Ω –¥–ª—è —Ñ–∞–π–ª–∞: {clean_file}"
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error processing smart filter: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_smart_filter_process_batch(self):
        """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ clean-—Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä"""
        try:
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length > 1024 * 1024:
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length).decode('utf-8')
            data = json.loads(post_data)

            filter_name = data.get('filter_name', 'italy_hydraulics')
            pattern = data.get('pattern', 'output/*_clean_*.txt')
            include_metadata = data.get('include_metadata', True)

            # –ó–∞–ø—É—Å–∫–∞–µ–º batch –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
            def run_smart_filter_batch():
                try:
                    from smart_filter_processor import SmartFilterProcessor

                    processor = SmartFilterProcessor(filter_name=filter_name)
                    results = processor.process_clean_batch(pattern=pattern)

                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚úÖ Batch smart filter –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} —Ñ–∞–π–ª–æ–≤"
                        })

                except Exception as error:
                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ùå –û—à–∏–±–∫–∞ batch smart filter: {str(error)}"
                        })

            import threading
            thread = threading.Thread(target=run_smart_filter_batch)
            thread.daemon = True
            thread.start()

            with processing_state["lock"]:
                processing_state["is_running"] = True
                processing_state["start_time"] = datetime.now()

            self.send_json_response({
                "success": True,
                "message": f"Batch smart filter –∑–∞–ø—É—â–µ–Ω (–ø–∞—Ç—Ç–µ—Ä–Ω: {pattern})"
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error processing batch smart filter: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_smart_filter_workflow(self):
        """–ü–æ–ª–Ω—ã–π workflow: LVP ‚Üí Base Filter ‚Üí Smart Filter ‚Üí Final CLEAN"""
        try:
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length > 1024 * 1024:
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length).decode('utf-8')
            data = json.loads(post_data)

            input_file = data.get('input_file')
            config_name = data.get('config_name', 'italy_hydraulics')
            score_threshold = float(data.get('score_threshold', 30.0))
            skip_base_filtering = data.get('skip_base_filtering', False)

            if not input_file:
                self.send_json_response({"error": "Missing input_file parameter"}, 400)
                return

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            from pathlib import Path
            validate_filename(Path(input_file).name)

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π workflow –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
            def run_workflow():
                try:
                    from smart_filter_workflow_manager import SmartFilterWorkflowManager
                    from dataclasses import asdict

                    # Progress callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                    def progress_callback(stage, progress, message):
                        with processing_state["lock"]:
                            processing_state["logs"].append({
                                "timestamp": datetime.now().strftime("%H:%M:%S"),
                                "message": f"[{stage}] {progress}%: {message}"
                            })

                    manager = SmartFilterWorkflowManager(progress_callback=progress_callback)
                    result = manager.execute_full_workflow(
                        input_file=Path(input_file),
                        config_name=config_name,
                        score_threshold=score_threshold,
                        skip_base_filtering=skip_base_filtering
                    )

                    with processing_state["lock"]:
                        processing_state["is_running"] = False

                        if result.overall_status == 'completed':
                            processing_state["logs"].append({
                                "timestamp": datetime.now().strftime("%H:%M:%S"),
                                "message": f"‚úÖ Workflow completed! Final files: {result.final_output_files.get('txt')}"
                            })
                        else:
                            processing_state["logs"].append({
                                "timestamp": datetime.now().strftime("%H:%M:%S"),
                                "message": f"‚ùå Workflow failed: {result.statistics.get('error', 'Unknown error')}"
                            })

                except Exception as error:
                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ùå Workflow error: {str(error)}"
                        })

            import threading
            thread = threading.Thread(target=run_workflow)
            thread.daemon = True
            thread.start()

            with processing_state["lock"]:
                processing_state["is_running"] = True
                processing_state["start_time"] = datetime.now()

            self.send_json_response({
                "success": True,
                "message": f"Full workflow started for: {input_file}",
                "config": config_name,
                "score_threshold": score_threshold
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error starting workflow: {e}")
            self.send_json_response({"error": str(e)}, 500)

    def handle_smart_filter_apply(self):
        """Apply smart filter configuration to recent clean files"""
        try:
            # Read request body
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length > 1024 * 1024:  # 1MB max
                self.send_json_response({"error": "Request too large"}, 413)
                return

            post_data = self.rfile.read(content_length).decode('utf-8')
            data = json.loads(post_data)

            # Extract config and validate
            config = data.get('config')
            if not config:
                self.send_json_response({"error": "Missing config parameter"}, 400)
                return

            # Validate config structure
            if not isinstance(config, dict):
                self.send_json_response({"error": "Config must be a dictionary"}, 400)
                return

            # Check for required config fields
            if 'metadata' not in config or 'name' not in config.get('metadata', {}):
                self.send_json_response({"error": "Config missing metadata.name field"}, 400)
                return

            config_name = config['metadata']['name']
            timestamp = data.get('timestamp')

            # Save config to smart_filters/configs/ directory
            from pathlib import Path
            import os

            config_dir = Path('smart_filters/configs')
            config_dir.mkdir(parents=True, exist_ok=True)

            # Sanitize config name for filename
            safe_config_name = "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in config_name.lower())
            config_filename = f"{safe_config_name}_config.json"
            config_path = config_dir / config_filename

            # Save config file
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)

            # Apply filter to recent clean files in background
            def run_apply_filter():
                try:
                    from smart_filter_processor_v2 import SmartFilterProcessor
                    from glob import glob

                    # Find recent clean files (last 7 days)
                    from datetime import datetime, timedelta
                    cutoff_time = datetime.now() - timedelta(days=7)

                    clean_files = []
                    for file_path in glob('output/*_clean_*.txt'):
                        file_stat = os.stat(file_path)
                        file_time = datetime.fromtimestamp(file_stat.st_mtime)
                        if file_time > cutoff_time:
                            clean_files.append(file_path)

                    if not clean_files:
                        with processing_state["lock"]:
                            processing_state["is_running"] = False
                            processing_state["logs"].append({
                                "timestamp": datetime.now().strftime("%H:%M:%S"),
                                "message": "‚ö†Ô∏è No recent clean files found (last 7 days)"
                            })
                        return

                    with processing_state["lock"]:
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"üìã Found {len(clean_files)} recent clean files to process"
                        })

                    # Process each file with the custom config
                    processor = SmartFilterProcessor(filter_name=safe_config_name)

                    # Load the custom config
                    processor.config = config

                    processed_count = 0
                    for clean_file in clean_files:
                        try:
                            result = processor.process_clean_file(
                                Path(clean_file),
                                include_metadata=True
                            )
                            processed_count += 1

                            with processing_state["lock"]:
                                processing_state["logs"].append({
                                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                                    "message": f"‚úÖ Processed: {Path(clean_file).name}"
                                })
                        except Exception as e:
                            with processing_state["lock"]:
                                processing_state["logs"].append({
                                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                                    "message": f"‚ùå Error processing {Path(clean_file).name}: {str(e)}"
                                })

                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚úÖ Filter applied to {processed_count}/{len(clean_files)} files"
                        })

                except Exception as error:
                    with processing_state["lock"]:
                        processing_state["is_running"] = False
                        processing_state["logs"].append({
                            "timestamp": datetime.now().strftime("%H:%M:%S"),
                            "message": f"‚ùå Error applying filter: {str(error)}"
                        })

            # Start background processing
            import threading
            thread = threading.Thread(target=run_apply_filter)
            thread.daemon = True
            thread.start()

            with processing_state["lock"]:
                processing_state["is_running"] = True
                processing_state["start_time"] = datetime.now()

            self.send_json_response({
                "success": True,
                "message": f"Filter '{config_name}' saved and applied to recent clean files",
                "config_name": config_name,
                "config_file": str(config_path),
                "timestamp": timestamp
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error applying smart filter: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    # ===== TEMPLATE API HANDLERS =====

    def convert_old_to_new_format(self, old_config, config_name):
        """
        Convert old Python backend format to new frontend format

        Args:
            old_config (dict): Old format config from smart_filters/configs/*.json
            config_name (str): Config filename (without .json)

        Returns:
            dict: New format config compatible with frontend
        """
        from datetime import datetime

        # Extract data from actual config format
        # Handle both 'display_name' and 'config_name'
        filter_name = old_config.get('display_name', old_config.get('config_name', config_name))
        version = old_config.get('version', '1.0')
        description = old_config.get('description', '')

        # Handle nested target_market structure
        target_market = old_config.get('target_market', {})
        target_country = target_market.get('country_name', 'Unknown')
        languages = target_market.get('language_codes', ['en'])

        target_industry = old_config.get('target_industry', 'Unknown')

        # Geographic data - handle both old and new formats
        geographic = old_config.get('geographic', {})
        geographic_priorities = old_config.get('geographic_priorities', {})

        # Use geographic_priorities if available (new format)
        priority_high = geographic_priorities.get('high', geographic.get('priority_high', []))
        priority_medium = geographic_priorities.get('medium', geographic.get('priority_medium', []))

        # Handle exclusions - new format uses nested structure
        exclusions = old_config.get('exclusions', {})
        excluded = exclusions.get('excluded_country_domains', geographic.get('excluded_countries', []))

        # Industry keywords - handle both old and new formats
        industry_kw = old_config.get('industry_keywords', {})
        keywords = old_config.get('keywords', {})

        # Map new keywords format to old format
        if keywords and not industry_kw:
            # Get first category as primary keywords (e.g., 'hydraulic_cylinders')
            categories = list(keywords.keys())
            primary_category = categories[0] if categories else None

            if primary_category and primary_category != 'oem_indicators':
                industry_kw['primary_positive'] = keywords.get(primary_category, [])

            # Get 'applications' or second category as secondary
            if 'applications' in keywords:
                industry_kw['secondary_positive'] = keywords.get('applications', [])
            elif len(categories) > 1:
                industry_kw['secondary_positive'] = keywords.get(categories[1], [])

            # Get OEM indicators
            if 'oem_indicators' in keywords:
                industry_kw['oem_indicators'] = keywords.get('oem_indicators', [])

        # Negative keywords - extract from exclusions.excluded_industries
        negative_kw = old_config.get('negative_keywords', [])
        if not negative_kw and 'excluded_industries' in exclusions:
            # Flatten all excluded industry keywords
            for category, terms in exclusions['excluded_industries'].items():
                negative_kw.extend(terms)

        # Get scoring config - use from config if available, otherwise defaults
        scoring_config = old_config.get('scoring', {})
        weights = scoring_config.get('weights', {
            "email_quality": 0.10,
            "company_relevance": 0.45,
            "geographic_priority": 0.30,
            "engagement": 0.15
        })
        thresholds = scoring_config.get('thresholds', {
            "high_priority": 100,
            "medium_priority": 50,
            "low_priority": 10
        })

        # Convert to new format
        new_config = {
            "metadata": {
                "id": config_name,
                "name": filter_name,
                "description": description,
                "version": version,
                "author": "system",
                "created": datetime.now().isoformat(),
                "updated": datetime.now().isoformat()
            },
            "target": {
                "country": target_country,
                "industry": target_industry,
                "languages": languages
            },
            "scoring": {
                "weights": weights,
                "thresholds": thresholds
            },
            "company_keywords": {
                "primary_keywords": {
                    "positive": [
                        {"term": term, "weight": 1.0}
                        for term in industry_kw.get('primary_positive', [])[:10]  # Limit to 10
                    ],
                    "negative": [
                        {"term": term, "weight": 0.5}
                        for term in (industry_kw.get('primary_negative', []) + negative_kw)[:10]
                    ]
                },
                "secondary_keywords": {
                    "positive": industry_kw.get('secondary_positive', [])[:10],
                    "negative": industry_kw.get('secondary_negative', [])[:10]
                }
            },
            "geographic_rules": {
                "target_regions": priority_high[:20],  # Limit to 20
                "exclude_regions": excluded[:20],
                "multipliers": {
                    # Default multipliers
                    target_country: 2.0,
                    "EU": 1.2,
                    "Others": 0.3
                }
            },
            "email_quality": {
                "corporate_domains": True,
                "free_email_penalty": -0.3,
                "structure_quality": True,
                "suspicious_patterns": []
            },
            "domain_rules": {
                "oemEquipment": {
                    "keywords": industry_kw.get('oem_indicators', [])[:10],
                    "multiplier": 1.3
                }
            }
        }

        return new_config

    def handle_get_templates(self):
        """Get all templates (built-in + user templates)"""
        try:
            # Load ALL configs from smart_filters/configs/ directory (UNIFIED)
            builtin_templates = {}
            configs_dir = Path('smart_filters/configs')

            if configs_dir.exists():
                print(f"üìÇ Loading templates from {configs_dir}...")
                for config_file in configs_dir.glob('*.json'):
                    try:
                        config_name = config_file.stem  # filename without .json

                        # Skip user_templates.json (will be loaded separately)
                        if config_name == 'user_templates':
                            continue

                        with open(config_file, 'r', encoding='utf-8') as f:
                            old_config = json.load(f)

                        # Convert old format to new format
                        new_config = self.convert_old_to_new_format(old_config, config_name)
                        builtin_templates[config_name] = new_config

                        print(f"  ‚úÖ Loaded: {new_config['metadata']['name']}")

                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Failed to load {config_file}: {e}")
                        continue

                print(f"‚úÖ Loaded {len(builtin_templates)} built-in templates")
            else:
                print(f"‚ö†Ô∏è Configs directory not found: {configs_dir}")
                # Fallback to hardcoded templates
                builtin_templates = {
                    "italy_hydraulics": {"source": "builtin", "metadata": {"name": "Italy Hydraulics"}},
                    "germany_manufacturing": {"source": "builtin", "metadata": {"name": "Germany Manufacturing"}},
                    "generic": {"source": "builtin", "metadata": {"name": "Generic Template"}}
                }

            # Load user templates from config/user_templates.json
            user_templates_file = Path('config/user_templates.json')
            user_templates = {}

            if user_templates_file.exists():
                try:
                    with open(user_templates_file, 'r', encoding='utf-8') as f:
                        user_templates = json.load(f)
                    print(f"‚úÖ Loaded {len(user_templates)} user templates")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load user templates: {e}")

            # Combine both
            all_templates = {
                "builtin": builtin_templates,
                "user": user_templates
            }

            self.send_json_response({
                "success": True,
                "templates": all_templates,
                "count": {
                    "builtin": len(builtin_templates),
                    "user": len(user_templates),
                    "total": len(builtin_templates) + len(user_templates)
                }
            })

        except Exception as e:
            print(f"‚ùå Error getting templates: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    def handle_save_template(self):
        """Save a new template or update existing one"""
        try:
            # Read request body
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length > 1024 * 1024:  # 1MB max for template
                self.send_json_response({"error": "Template too large (max 1MB)"}, 413)
                return

            post_data = self.rfile.read(content_length).decode('utf-8')
            data = json.loads(post_data)

            template_id = data.get('id')
            template = data.get('template')

            if not template_id or not template:
                self.send_json_response({"error": "Missing id or template"}, 400)
                return

            # Validate template has required fields
            if not isinstance(template, dict):
                self.send_json_response({"error": "Template must be a dictionary"}, 400)
                return

            if 'metadata' not in template:
                self.send_json_response({"error": "Template missing metadata field"}, 400)
                return

            # Ensure config directory exists
            Path('config').mkdir(exist_ok=True)

            # Load existing templates
            templates_file = Path('config/user_templates.json')
            templates = {}

            if templates_file.exists():
                with open(templates_file, 'r', encoding='utf-8') as f:
                    templates = json.load(f)

            # Add/update template
            templates[template_id] = template

            # Save to file
            with open(templates_file, 'w', encoding='utf-8') as f:
                json.dump(templates, f, indent=2, ensure_ascii=False)

            self.send_json_response({
                "success": True,
                "message": f"Template '{template.get('metadata', {}).get('name', template_id)}' saved successfully",
                "id": template_id
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error saving template: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    def handle_delete_template(self):
        """Delete a template by ID"""
        try:
            # Extract template ID from path: /api/templates/:id
            path = urlparse(self.path).path
            template_id = path.split('/api/templates/', 1)[1]

            if not template_id:
                self.send_json_response({"error": "Missing template ID"}, 400)
                return

            # Load existing templates
            templates_file = Path('config/user_templates.json')

            if not templates_file.exists():
                self.send_json_response({"error": "No templates found"}, 404)
                return

            with open(templates_file, 'r', encoding='utf-8') as f:
                templates = json.load(f)

            # Check if template exists
            if template_id not in templates:
                self.send_json_response({"error": f"Template '{template_id}' not found"}, 404)
                return

            # Delete template
            template_name = templates[template_id].get('metadata', {}).get('name', template_id)
            del templates[template_id]

            # Save updated templates
            with open(templates_file, 'w', encoding='utf-8') as f:
                json.dump(templates, f, indent=2, ensure_ascii=False)

            self.send_json_response({
                "success": True,
                "message": f"Template '{template_name}' deleted successfully"
            })

        except Exception as e:
            print(f"Error deleting template: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    def handle_save_draft(self):
        """Save draft for a component (auto-save)"""
        try:
            # Read request body
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length > 1024 * 1024:  # 1MB max for draft
                self.send_json_response({"error": "Draft too large (max 1MB)"}, 413)
                return

            post_data = self.rfile.read(content_length).decode('utf-8')
            data = json.loads(post_data)

            component = data.get('component')  # 'wizard', 'visual_builder', or 'json_editor'
            draft = data.get('draft')

            if not component:
                self.send_json_response({"error": "Missing component parameter"}, 400)
                return

            # Validate component name
            allowed_components = ['wizard', 'visual_builder', 'json_editor']
            if component not in allowed_components:
                self.send_json_response({"error": f"Invalid component. Must be one of: {', '.join(allowed_components)}"}, 400)
                return

            # Ensure drafts directory exists
            Path('config/drafts').mkdir(parents=True, exist_ok=True)

            # Save draft
            draft_file = Path(f'config/drafts/{component}_draft.json')

            with open(draft_file, 'w', encoding='utf-8') as f:
                json.dump(draft, f, indent=2, ensure_ascii=False)

            self.send_json_response({
                "success": True,
                "message": f"Draft saved for {component}",
                "component": component,
                "timestamp": datetime.now().isoformat()
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Invalid JSON"}, 400)
        except Exception as e:
            print(f"Error saving draft: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_draft(self):
        """Get draft for a component"""
        try:
            # Extract component from path: /api/templates/draft/:component
            path = urlparse(self.path).path
            component = path.split('/api/templates/draft/', 1)[1]

            if not component:
                self.send_json_response({"error": "Missing component parameter"}, 400)
                return

            # Validate component name
            allowed_components = ['wizard', 'visual_builder', 'json_editor']
            if component not in allowed_components:
                self.send_json_response({"error": f"Invalid component. Must be one of: {', '.join(allowed_components)}"}, 400)
                return

            # Load draft
            draft_file = Path(f'config/drafts/{component}_draft.json')

            if not draft_file.exists():
                self.send_json_response({
                    "success": True,
                    "draft": None,
                    "message": "No draft found"
                })
                return

            with open(draft_file, 'r', encoding='utf-8') as f:
                draft = json.load(f)

            # Get file modification time
            mod_time = datetime.fromtimestamp(draft_file.stat().st_mtime)

            self.send_json_response({
                "success": True,
                "draft": draft,
                "component": component,
                "saved_at": mod_time.isoformat()
            })

        except json.JSONDecodeError:
            self.send_json_response({"error": "Draft file is corrupted"}, 500)
        except Exception as e:
            print(f"Error getting draft: {e}")
            import traceback
            traceback.print_exc()
            self.send_json_response({"error": str(e)}, 500)

    # ===== BLOCKLIST API HANDLERS =====

    def handle_get_blocklist(self):
        """GET /api/blocklist"""
        try:
            response = handle_get_blocklist()
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_get_blocklist_stats(self):
        """GET /api/blocklist/stats"""
        try:
            response = handle_get_blocklist_stats()
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_blocklist_search(self):
        """GET /api/blocklist/search?q=query"""
        try:
            parsed = urlparse(self.path)
            params = parse_qs(parsed.query)
            query = params.get('q', [''])[0]

            response = handle_blocklist_search(query)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_blocklist_export(self):
        """GET /api/blocklist/export?format=json"""
        try:
            parsed = urlparse(self.path)
            params = parse_qs(parsed.query)
            format_type = params.get('format', ['json'])[0]

            response = handle_blocklist_export(format_type)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_post_blocklist_add(self):
        """POST /api/blocklist/add"""
        try:
            content_length = int(self.headers['Content-Length'])
            body = self.rfile.read(content_length)
            data = json.loads(body.decode('utf-8'))

            response = handle_blocklist_add(data)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_post_blocklist_remove(self):
        """POST /api/blocklist/remove"""
        try:
            content_length = int(self.headers['Content-Length'])
            body = self.rfile.read(content_length)
            data = json.loads(body.decode('utf-8'))

            response = handle_blocklist_remove(data)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_post_blocklist_bulk_add(self):
        """POST /api/blocklist/bulk-add"""
        try:
            content_length = int(self.headers['Content-Length'])
            body = self.rfile.read(content_length)
            data = json.loads(body.decode('utf-8'))

            response = handle_blocklist_bulk_add(data)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_post_blocklist_bulk_remove(self):
        """POST /api/blocklist/bulk-remove"""
        try:
            content_length = int(self.headers['Content-Length'])
            body = self.rfile.read(content_length)
            data = json.loads(body.decode('utf-8'))

            response = handle_blocklist_bulk_remove(data)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)

    def handle_post_blocklist_import_csv(self):
        """POST /api/blocklist/import-csv"""
        try:
            content_length = int(self.headers['Content-Length'])
            body = self.rfile.read(content_length)
            data = json.loads(body.decode('utf-8'))

            response = handle_blocklist_import_csv(data)
            self.send_json_response(response)
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)


def run_server(port=8080):
    """–ó–∞–ø—É—Å–∫ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –ø–æ—Ä—Ç–∞"""
    max_port = port + 100
    server = None
    ws_thread = None

    while port <= max_port:
        try:
            server_address = ('', port)
            server = HTTPServer(server_address, EmailCheckerWebHandler)

            # –ó–∞–ø—É—Å–∫ WebSocket —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ –Ω–∞ —Å–æ—Å–µ–¥–Ω–µ–º –ø–æ—Ä—Ç—É
            ws_port = port + 1
            ws_thread = threading.Thread(
                target=websocket_server.run_websocket_server,
                args=("0.0.0.0", ws_port),
                daemon=True,
                name="WebSocketServer"
            )
            ws_thread.start()

            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ WebSocket
            time.sleep(0.5)

            print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë     Email List Manager Web Server + WebSocket üöÄ        ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                          ‚ïë
‚ïë  HTTP Server: http://localhost:{port:<28} ‚ïë
‚ïë  WebSocket:   ws://localhost:{ws_port}/ws{' ' * 19} ‚ïë
‚ïë                                                          ‚ïë
‚ïë  üì± –ò–ù–¢–ï–†–§–ï–ô–°–´:                                         ‚ïë
‚ïë  ‚úì –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é):                        ‚ïë
‚ïë    http://localhost:{port:<38} ‚ïë
‚ïë                                                          ‚ïë
‚ïë  ‚úì –ù–æ–≤—ã–π (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π):                          ‚ïë
‚ïë    http://localhost:{port}/new{' ' * 32} ‚ïë
‚ïë                                                          ‚ïë
‚ïë  ‚úÖ Real-time –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–∫–ª—é—á–µ–Ω—ã —á–µ—Ä–µ–∑ WebSocket       ‚ïë
‚ïë                                                          ‚ïë
‚ïë  –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C                          ‚ïë
‚ïë                                                          ‚ïë
‚ïë  üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:                                       ‚ïë
‚ïë  ‚úì Command Injection –∑–∞—â–∏—Ç–∞                            ‚ïë
‚ïë  ‚úì Path Traversal –∑–∞—â–∏—Ç–∞                               ‚ïë
‚ïë  ‚úì –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö                       ‚ïë
‚ïë  ‚úì –ë–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏ –∫–æ–º–∞–Ω–¥ –∏ endpoints                     ‚ïë
‚ïë                                                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
            """)
            break
        except OSError as e:
            if e.errno == 48 or e.errno == 10048:  # Port already in use
                print(f"–ü–æ—Ä—Ç {port} –∑–∞–Ω—è—Ç, –ø—Ä–æ–±—É—é {port + 1}...")
                port += 1
            else:
                raise

    if server:
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            print("\n\n–°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            server.shutdown()
    else:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä. –í—Å–µ –ø–æ—Ä—Ç—ã –æ—Ç {port-100} –¥–æ {max_port} –∑–∞–Ω—è—Ç—ã.")

if __name__ == "__main__":
    import sys

    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    port = 8089
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            print(f"–ù–µ–≤–µ—Ä–Ω—ã–π –ø–æ—Ä—Ç: {sys.argv[1]}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Ä—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8080")

    run_server(port)