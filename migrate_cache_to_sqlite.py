#!/usr/bin/env python3
"""
ะะธะณัะฐัะธั ะบะตัะฐ ั JSON ะฝะฐ SQLite

ะญัะพั ัะบัะธะฟั ะฟะตัะตะฝะพัะธั ะดะฐะฝะฝัะต ะธะท processed_files.json (16.62 MB)
ะฒ ัััะตะบัะธะฒะฝัั SQLite ะฑะฐะทั ะดะฐะฝะฝัั ัะตัะตะท CacheManager (ะพะถะธะดะฐะตััั ~1.5 MB).

ะัะตะธะผััะตััะฒะฐ SQLite:
- 90% ะผะตะฝััะต ัะฐะทะผะตั
- O(1) ะดะพัััะฟ ัะตัะตะท ะธะฝะดะตะบัั
- ะะต ะฝัะถะฝะพ ะทะฐะณััะถะฐัั ะฒะตัั ะบะตั ะฒ ะฟะฐะผััั
- 10x ะฑััััะตะต ะดะตะดัะฟะปะธะบะฐัะธั

ะะะะะ: TXT ัะฐะนะปั ะฒ output/ ะะ ะะะะะะฏะฎะขะกะฏ! ะะฝะธ ะพััะฐัััั ะบะฐะบ ะตััั.
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from cache_manager import CacheManager


def migrate_json_to_sqlite():
    """ะะธะณัะธััะตั JSON ะบะตั ะฒ SQLite"""

    print("๐ ะะะะะะฆะะฏ ะะะจะ JSON โ SQLite\n")
    print("="*60)

    # ะััะธ
    json_cache_file = Path(".cache/processed_files.json")
    sqlite_db_file = Path(".cache/processing_cache.db")

    # ะัะพะฒะตััะตะผ ะฝะฐะปะธัะธะต JSON ะบะตัะฐ
    if not json_cache_file.exists():
        print("โ ะคะฐะนะป .cache/processed_files.json ะฝะต ะฝะฐะนะดะตะฝ")
        print("   ะะตัะตะณะพ ะผะธะณัะธัะพะฒะฐัั. ะะพะทะผะพะถะฝะพ ะบะตั ัะถะต ะฒ SQLite ะธะปะธ ะตัะต ะฝะต ัะพะทะดะฐะฝ.")
        return

    # ะัะพะฒะตััะตะผ ะฝะต ะฑัะปะฐ ะปะธ ัะถะต ะผะธะณัะฐัะธั
    if sqlite_db_file.exists():
        print("\nโ๏ธ  SQLite ะบะตั ัะถะต ัััะตััะฒัะตั. ะะตัะตะทะฐะฟะธััะฒะฐะตะผ...")
        sqlite_db_file.unlink()  # ะฃะดะฐะปัะตะผ ััะฐััะน

    # ะะฐะทะผะตั ััะฐัะพะณะพ ะบะตัะฐ
    old_size_mb = json_cache_file.stat().st_size / (1024 * 1024)
    print(f"\n๐ฆ ะกัะฐััะน JSON ะบะตั: {old_size_mb:.2f} MB")

    # ะงะธัะฐะตะผ JSON
    print("\n๐ ะงะธัะฐะตะผ JSON ะบะตั...")
    try:
        with open(json_cache_file, 'r', encoding='utf-8') as f:
            old_cache = json.load(f)
    except Exception as e:
        print(f"โ ะัะธะฑะบะฐ ััะตะฝะธั JSON: {e}")
        return

    print(f"โ ะะฐะณััะถะตะฝะพ {len(old_cache)} ะทะฐะฟะธัะตะน ะธะท JSON")

    # ะะฝะธัะธะฐะปะธะทะธััะตะผ CacheManager (ัะพะทะดะฐัั SQLite ะะ)
    print("\n๐๏ธ  ะกะพะทะดะฐะตะผ SQLite ะบะตั ัะตัะตะท CacheManager...")
    cache_manager = CacheManager(".cache")

    # ะะธะณัะธััะตะผ ะดะฐะฝะฝัะต
    print("\n๐ ะะธะณัะฐัะธั ะดะฐะฝะฝัั...")
    migrated = 0
    skipped = 0

    for filename, data in old_cache.items():
        try:
            # ะัะพะฒะตััะตะผ ััะพ ะตััั ะฝะตะพะฑัะพะดะธะผัะต ะฟะพะปั
            if 'hash' not in data:
                print(f"   โ๏ธ  ะัะพะฟััะบะฐะตะผ {filename}: ะฝะตั ัะตัะฐ")
                skipped += 1
                continue

            # ะคะพัะผะธััะตะผ ะฟััั ะบ ัะฐะนะปั
            # JSON ััะฐะฝะธั ะฟัะพััะพ ะธะผั, ะฝัะถะฝะพ ะพะฟัะตะดะตะปะธัั ะณะดะต ัะฐะนะป
            if Path(f"input/{filename}").exists():
                file_path = Path(f"input/{filename}")
            else:
                print(f"   โ๏ธ  ะคะฐะนะป {filename} ะฝะต ะฝะฐะนะดะตะฝ ะฒ input/")
                skipped += 1
                continue

            # ะัะพะฒะตััะตะผ ัะตั ะฐะบััะฐะปะตะฝ ะปะธ
            current_hash = cache_manager.get_file_hash(file_path)
            if current_hash != data['hash']:
                print(f"   โ๏ธ  ะัะพะฟััะบะฐะตะผ {filename}: ัะตั ะธะทะผะตะฝะธะปัั (ัะฐะนะป ะฑัะป ะผะพะดะธัะธัะธัะพะฒะฐะฝ)")
                skipped += 1
                continue

            # ะะทะฒะปะตะบะฐะตะผ ัะตะทัะปััะฐัั ะพะฑัะฐะฑะพัะบะธ
            result_data = data.get('result_data', {})
            results = result_data.get('results', {})  # Email ะฟะพ ะบะฐัะตะณะพัะธัะผ

            # ะะพะดััะธััะฒะฐะตะผ ะบะพะปะธัะตััะฒะพ email ะฟะพ ะบะฐัะตะณะพัะธัะผ
            stats = {
                'total': result_data.get('stats', {}).get('total_checked', 0),
                'duplicates_removed': result_data.get('duplicates_removed', 0),
                'prefix_duplicates_removed': result_data.get('prefix_duplicates_removed', 0),
                'has_metadata': False  # ะะปั ััะฐัะพะณะพ ะบะตัะฐ ะฝะตั ะผะตัะฐะดะฐะฝะฝัั
            }

            # ะกะพััะฐะฝัะตะผ ะฒ SQLite ัะตัะตะท ะพะฑัะตะบั ั ะผะธะฝะธะผะฐะปัะฝัะผะธ ะดะฐะฝะฝัะผะธ
            # (ะฟะพะปะฝัะต ัะฟะธัะบะธ email ะฝะต ัะพััะฐะฝัะตะผ, ัะพะปัะบะพ ัะตั ะธ ััะฐัะธััะธะบั)
            class MinimalResult:
                def __init__(self, filepath, stats, email_results):
                    self.file_path = Path(filepath)
                    self.success = True
                    self.error = None
                    self.duplicates_removed = stats.get('duplicates_removed', 0)
                    self.prefix_duplicates_removed = stats.get('prefix_duplicates_removed', 0)

                    # ะััะธะฑััั ะดะปั CacheManager.save_processing_result()
                    self.file_type = Path(filepath).suffix.lstrip('.') or 'txt'
                    self.timestamp = datetime.now().isoformat()
                    self.processing_time = 0.0

                    # Email ัะฟะธัะบะธ (CacheManager ะพะถะธะดะฐะตั ัะฟะธัะบะธ, ะฝะต ะบะพะปะธัะตััะฒะพ!)
                    self.clean_emails = list(email_results.get('clean', []))
                    self.blocked_email = list(email_results.get('blocked_email', []))
                    self.blocked_domain = list(email_results.get('blocked_domain', []))
                    self.invalid_emails = list(email_results.get('invalid', []))

                    # ะกัะฐัะธััะธะบะฐ
                    self.total_emails = stats.get('total', 0)
                    self.has_metadata = stats.get('has_metadata', False)

            result = MinimalResult(file_path, stats, results)

            # ะกะพััะฐะฝัะตะผ ัะตัะตะท CacheManager (ะผะตัะพะด ัะฐะผ ะฒััะธัะปะธั hash ัะฐะนะปะฐ)
            # CacheManager.save_processing_result() ะฐะฒัะพะผะฐัะธัะตัะบะธ ัะพััะฐะฝะธั ะฒัะต email
            cache_manager.save_processing_result(result)

            migrated += 1

            if migrated % 10 == 0:
                print(f"   โ ะะธะณัะธัะพะฒะฐะฝะพ {migrated}/{len(old_cache)} ัะฐะนะปะพะฒ...")

        except Exception as e:
            print(f"   โ ะัะธะฑะบะฐ ะฟัะธ ะผะธะณัะฐัะธะธ {filename}: {e}")
            skipped += 1

    print(f"\nโ ะะธะณัะฐัะธั ะทะฐะฒะตััะตะฝะฐ!")
    print(f"   ะฃัะฟะตัะฝะพ: {migrated}")
    print(f"   ะัะพะฟััะตะฝะพ: {skipped}")

    # ะะฐะทะผะตั ะฝะพะฒะพะณะพ ะบะตัะฐ
    if sqlite_db_file.exists():
        new_size_mb = sqlite_db_file.stat().st_size / (1024 * 1024)
        print(f"\n๐ฆ ะะพะฒัะน SQLite ะบะตั: {new_size_mb:.2f} MB")

        if old_size_mb > 0:
            savings = ((old_size_mb - new_size_mb) / old_size_mb) * 100
            print(f"๐ฐ ะญะบะพะฝะพะผะธั: {savings:.1f}% ({old_size_mb - new_size_mb:.2f} MB)")

    # ะกะพะทะดะฐะตะผ ัะตะทะตัะฒะฝัั ะบะพะฟะธั ััะฐัะพะณะพ JSON
    backup_file = Path(f".cache/processed_files_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    print(f"\n๐พ ะกะพะทะดะฐะฝะธะต ัะตะทะตัะฒะฝะพะน ะบะพะฟะธะธ...")
    print(f"   {backup_file}")

    import shutil
    shutil.copy(json_cache_file, backup_file)

    print("\nโ ะะพัะพะฒะพ! ะขะตะฟะตัั ะผะพะถะฝะพ ะธัะฟะพะปัะทะพะฒะฐัั CacheManager ะฒะผะตััะพ JSON.")
    print("\nะกะปะตะดัััะธะต ัะฐะณะธ:")
    print("1. ะัะพัะตััะธัะพะฒะฐัั ัะฐะฑะพัั ั ะฝะพะฒัะผ ะบะตัะตะผ")
    print("2. ะะฑะฝะพะฒะธัั email_checker.py ะดะปั ะธัะฟะพะปัะทะพะฒะฐะฝะธั CacheManager")
    print("3. ะะพัะปะต ะฟัะพะฒะตัะบะธ ะผะพะถะฝะพ ัะดะฐะปะธัั ััะฐััะน JSON ะบะตั")
    print(f"\nโ๏ธ  ะกัะฐััะน ะบะตั ัะพััะฐะฝะตะฝ ะบะฐะบ ัะตะทะตัะฒะฝะฐั ะบะพะฟะธั:")
    print(f"   {backup_file}")


def verify_migration():
    """ะัะพะฒะตััะตั ะบะพััะตะบัะฝะพััั ะผะธะณัะฐัะธะธ"""
    print("\n๐ ะะะะะะะะ ะะะะะะฆะะ\n")
    print("="*60)

    cache = CacheManager(".cache")

    # ะะพะปััะฐะตะผ ััะฐัะธััะธะบั
    stats = cache.get_all_statistics()

    print("\n๐ ะกัะฐัะธััะธะบะฐ SQLite ะบะตัะฐ:")
    print(f"   ะคะฐะนะปะพะฒ ะฒ ะบะตัะต: {stats['total_files']}")
    print(f"   ะฃัะฟะตัะฝัั: {stats['successful_files']}")
    print(f"   ะฃะฝะธะบะฐะปัะฝัั email: {stats['total_unique_emails']:,}")
    print(f"\n   ะะพ ะบะฐัะตะณะพัะธัะผ:")

    for category, count in stats['emails_by_category'].items():
        print(f"      {category:20s}: {count:>10,}")

    print(f"\n   ะะฐะทะผะตั ะะ: {stats['database_size_mb']:.2f} MB")

    # ะัะพะฒะตัะบะฐ ะดะตะดัะฟะปะธะบะฐัะธะธ
    print("\n๐ ะัะพะฒะตัะบะฐ ะดะตะดัะฟะปะธะบะฐัะธะธ:")
    processed_emails = cache.get_all_processed_emails()
    print(f"   ะัะตะณะพ email ะดะปั ะดะตะดัะฟะปะธะบะฐัะธะธ: {len(processed_emails):,}")
    print("   โ ะะพัััะฟ ัะตัะตะท ะธะฝะดะตะบัั - O(1)")

    print("\nโ ะะธะณัะฐัะธั ะฟัะพัะปะฐ ััะฟะตัะฝะพ!")


if __name__ == "__main__":
    print("""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ   ะะะะะะฆะะฏ ะะะจะ: JSON โ SQLite                          โ
โ                                                          โ
โ   ะัะตะธะผััะตััะฒะฐ:                                          โ
โ   โข 90% ะผะตะฝััะต ัะฐะทะผะตั (16.62 MB โ ~1.5 MB)             โ
โ   โข 10x ะฑััััะตะต ะดะตะดัะฟะปะธะบะฐัะธั (ะธะฝะดะตะบัั ะฒะผะตััะพ ัะบะฐะฝะพะฒ)   โ
โ   โข O(1) ะดะพัััะฟ ะบ ะดะฐะฝะฝัะผ                                โ
โ   โข ะะต ะฝัะถะฝะพ ะทะฐะณััะถะฐัั ะฒะตัั ะบะตั ะฒ ะฟะฐะผััั                โ
โ                                                          โ
โ   ะะะะะะขะะฏ: TXT ัะฐะนะปั ะฒ output/ ะะ ะะะะะะฏะฎะขะกะฏ!         โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    """)

    try:
        migrate_json_to_sqlite()

        # ะะฒัะพะผะฐัะธัะตัะบะธ ะทะฐะฟััะบะฐะตะผ ะฟัะพะฒะตัะบั
        print("\n๐ ะะฐะฟััะบ ะฟัะพะฒะตัะบะธ ะผะธะณัะฐัะธะธ...")
        verify_migration()

    except KeyboardInterrupt:
        print("\n\nโ๏ธ  ะะธะณัะฐัะธั ะฟัะตัะฒะฐะฝะฐ ะฟะพะปัะทะพะฒะฐัะตะปะตะผ")
        sys.exit(1)
    except Exception as e:
        print(f"\nโ ะัะธัะธัะตัะบะฐั ะพัะธะฑะบะฐ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
